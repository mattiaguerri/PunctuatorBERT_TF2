{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore Punctuation In An Unpunctuated Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import loadFile\n",
    "from dataProcessing import encodeDataInfer, insertTarget, processingScriber, processingOPUS\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFCamembertForMaskedLM\n",
    "from datetime import datetime\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jplu/tf-camembert-base\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### path to weights\n",
    "# checkpointPath = \"Models/20200530_161559/cp-001.ckpt\"  # baseline model\n",
    "checkpointPath = \"Models/20200601_090641/cp-006.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### punctuation decoder\n",
    "punDec = {\n",
    "    \"0\": \"SPACE\",\n",
    "    \"1\": \"PERIOD\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 32005\n",
    "segment_size = 32\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of dataset with sentences\n",
    "dataSetName = \"./DataScriber/raw.processed.Valid_01_extractNoPun.txt\"\n",
    "\n",
    "data = loadFile(dataSetName)\n",
    "\n",
    "X_ = encodeDataInfer(data, tokenizer)\n",
    "X = insertTarget(X_, segment_size)\n",
    "\n",
    "# ### Get Only A Fraction Of Dataset\n",
    "# n = 320\n",
    "# X = X[0:n]\n",
    "\n",
    "# instantiate tf.data.Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X,)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Of X_ =  9155\n",
      "Shape Of X   =  (9155, 32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Length Of X_ = \", len(X_))\n",
    "print(\"Shape Of X   = \", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>### Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFCamembertForMaskedLM.\n",
      "\n",
      "All the weights of TFCamembertForMaskedLM were initialized from the model checkpoint at jplu/tf-camembert-base.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFCamembertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_input = tf.keras.Input(shape=(segment_size), dtype='int32', name='bert_input')\n",
    "x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(bert_input)[0]\n",
    "x = tf.keras.layers.Reshape((segment_size*vocab_size,))(x)\n",
    "dense_out = tf.keras.layers.Dense(len(punDec))(x)\n",
    "model = tf.keras.Model(bert_input, dense_out, name='CamemBERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd6b191ac40>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the weights\n",
    "model.load_weights(checkpointPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feats = next(iter(dataset))\n",
    "preds = np.argmax(model.predict(dataset), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9155\n"
     ]
    }
   ],
   "source": [
    "print(len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return The Text With Restored (Inferred) Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restorePunctuation(X, preds, punDec, tokenizer, fileName):\n",
    "    file = open(fileName, 'w')\n",
    "    for i in range(len(preds)):\n",
    "        word = tokenizer.convert_ids_to_tokens(X_[i])\n",
    "        pun = punDec[str(preds[i])]\n",
    "        file.write(word + \" | \" + pun + \" \\n\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "restorePunctuation(X_, preds, punDec, tokenizer, 'textRestored_01.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvExp00",
   "language": "python",
   "name": ".venvexp00"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
