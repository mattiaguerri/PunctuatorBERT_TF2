{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set a double strategy. Train TOP LAYER + FULL MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()  # silence TF warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import loadFile\n",
    "from dataProcessing import encodeData, insertTarget, processingScriber, processingOPUS\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFCamembertForMaskedLM\n",
    "from datetime import datetime\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jplu/tf-camembert-base\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### punctuation encoder\n",
    "punctuationEnc = {\n",
    "    'SPACE': 0,\n",
    "    'PERIOD': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabSize = 32005\n",
    "sequenceSize = 32\n",
    "batchSize = 32\n",
    "learningRate = 1e-5\n",
    "numEpoTop = 5\n",
    "numEpoAll = 7\n",
    "\n",
    "listHyper0 = ['vocabSize', 'sequenceSize', 'batchSize', 'learningRate', 'numEpoTop', 'NumEpoAll']\n",
    "listHyper1 = [str(vocabSize), str(sequenceSize), str(batchSize), str(learningRate),\n",
    "              str(numEpoTop), str(numEpoAll)]\n",
    "\n",
    "time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "save_path = 'ModelsExpScriber/{}/'.format(time)\n",
    "os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Data ... \n",
      "\n",
      "Training Dataset Tensor Shape =  (255, 32)\n"
     ]
    }
   ],
   "source": [
    "### Training Dataset\n",
    "print('\\nProcessing Data ... ')\n",
    "\n",
    "# THIS IN CASE STARTING FROM FILE WITH SENTENCES\n",
    "# this is the file structured in sentences\n",
    "trainDataName = \"./DataScriber/raw.processed.Train_01.txt\"\n",
    "# from sentences to columns words+punctuation\n",
    "dataTrain = loadFile(processingScriber(trainDataName))\n",
    "\n",
    "# # THIS IN CASE STARTING FROM FILE WITH COLUMNS\n",
    "# trainDataName = './AudioFeatures/outFile_05.txt'\n",
    "# dataTrain = loadFile(trainDataName)\n",
    "\n",
    "### Encode Data and insert target\n",
    "XTrain, yTrain = encodeData(dataTrain, tokenizer, punctuationEnc)\n",
    "\n",
    "### Create Sequences With The Target\n",
    "XTrainMod = insertTarget(XTrain, sequenceSize)\n",
    "\n",
    "# build the datasets\n",
    "dataTrainBuilt = tf.data.Dataset.from_tensor_slices((XTrainMod, yTrain)).shuffle(buffer_size=500000).batch(batchSize)\n",
    "\n",
    "print(\"\\nTraining Dataset Tensor Shape = \", XTrainMod.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BUILD THE MODEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFCamembertForMaskedLM.\n",
      "\n",
      "All the weights of TFCamembertForMaskedLM were initialized from the model checkpoint at jplu/tf-camembert-base.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFCamembertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "print('\\nBUILD THE MODEL')\n",
    "\n",
    "bert_input = tf.keras.Input(shape=(sequenceSize), dtype='int32', name='bert_input')\n",
    "x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(bert_input)[0]\n",
    "x = tf.keras.layers.Reshape((sequenceSize*vocabSize,))(x)\n",
    "dense_out = tf.keras.layers.Dense(len(punctuationEnc))(x)\n",
    "\n",
    "model = tf.keras.Model(bert_input, dense_out, name='model')\n",
    "\n",
    "# define the loss\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "def loss(model, x, y):\n",
    "    y_ = model(x)\n",
    "    return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "# func to calculate the gradients\n",
    "def grad(model, inputs, targets, trainLayerIndex):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables[trainLayerIndex:])\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop. TOP LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXPERIMENT FOLDER:  20200819_000417\n",
      "\n",
      "HYPERPARAMETERS\n",
      "\n",
      "Sequence Size =  32\n",
      "Batch Size =  32\n",
      "numEpoTop =  2\n",
      "numEpoAll =  2\n",
      "\n",
      "TRAINING DATASET TENSOR SHAPE =  (255, 32)\n",
      "\n",
      "TRAINING, TOP LAYER ONLY\n",
      "\n",
      "Epoch 001: (Training)   Loss: 6.669, Accuracy: 97.255%\n",
      "\n",
      "Epoch 002: (Training)   Loss: 2.336, Accuracy: 93.725%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEXPERIMENT FOLDER: \", time)\n",
    "\n",
    "print(\"\\nHYPERPARAMETERS\")\n",
    "print(\"\\nSequence Size = \", sequenceSize)\n",
    "print(\"Batch Size = \", batchSize)\n",
    "print(\"numEpoTop = \", numEpoTop)\n",
    "print(\"numEpoAll = \", numEpoAll)\n",
    "\n",
    "print(\"\\nTRAINING DATASET TENSOR SHAPE = \", XTrainMod.shape)\n",
    "\n",
    "print(\"\\nTRAINING, TOP LAYER ONLY\")\n",
    "\n",
    "trainLayerIndex = -2  # top layer only\n",
    "\n",
    "epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "checkpoint_path = save_path + \"cp-{epoch:03d}.ckpt\"\n",
    "\n",
    "tmpTrain = np.inf\n",
    "for epoch in range(1, numEpoTop+1):\n",
    "\n",
    "    # training loop\n",
    "    for x, y in dataTrainBuilt :\n",
    "        # optimize the model\n",
    "        loss_value, grads = grad(model, x, y, trainLayerIndex)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables[trainLayerIndex:]))\n",
    "\n",
    "        # track progress\n",
    "        epoch_loss_avg.update_state(loss_value)\n",
    "        epoch_accuracy.update_state(y, model(x))\n",
    "\n",
    "    # end epoch\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "    print(\"\\nEpoch {:03d}: (Training)   Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch, epoch_loss_avg.result(), epoch_accuracy.result()))\n",
    "\n",
    "    # # save model if new min for train loss is found\n",
    "    # if epoch_loss_avg.result().numpy() < tmpTrain:\n",
    "    tmpTrain = epoch_loss_avg.result().numpy()\n",
    "    model.save_weights(checkpoint_path.format(epoch=epoch))\n",
    "\n",
    "    epoch_loss_avg.reset_states()\n",
    "    epoch_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop. FULL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING, FULL MODEL\n",
      "\n",
      "Epoch 003: (Training)   Loss: 1.128, Accuracy: 97.647%\n",
      "\n",
      "Epoch 004: (Training)   Loss: 0.300, Accuracy: 95.294%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTRAINING, FULL MODEL\")\n",
    "\n",
    "trainLayerIndex = 0  # full model\n",
    "\n",
    "tmpTrain = np.inf\n",
    "for epoch in range(numEpoTop+1, numEpoTop+numEpoAll+1):\n",
    "\n",
    "    # training loop\n",
    "    for x, y in dataTrainBuilt:\n",
    "        # optimize the model\n",
    "        loss_value, grads = grad(model, x, y, trainLayerIndex)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables[trainLayerIndex:]))\n",
    "\n",
    "        # track progress\n",
    "        epoch_loss_avg.update_state(loss_value)\n",
    "        epoch_accuracy.update_state(y, model(x))\n",
    "\n",
    "    # end epoch\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "    print(\"\\nEpoch {:03d}: (Training)   Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch, epoch_loss_avg.result(), epoch_accuracy.result()))\n",
    "\n",
    "    # # save model if new min for train loss is found\n",
    "    # if epoch_loss_avg.result().numpy() < tmpTrain:\n",
    "    tmpTrain = epoch_loss_avg.result().numpy()\n",
    "    model.save_weights(checkpoint_path.format(epoch=epoch))\n",
    "\n",
    "    epoch_loss_avg.reset_states()\n",
    "    epoch_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Training Details On Log File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameLogFile = 'log.txt'\n",
    "logFile = open(save_path + nameLogFile, \"w\")\n",
    "\n",
    "# write name of model\n",
    "logFile.write(\"\\n\" + time + \"\\n\\n\")\n",
    "\n",
    "# write hyper parameters\n",
    "for i in range(len(listHyper0)):\n",
    "    logFile.write(listHyper0[i] + \":  \" + listHyper1[i] + \"\\n\")\n",
    "\n",
    "# write training details\n",
    "logFile.write('\\nTRAINING')\n",
    "trainLossArr = np.asarray(train_loss_results)\n",
    "trainAccArr = np.asarray(train_accuracy_results)\n",
    "for i in range(numEpoTop+numEpoAll):\n",
    "    logFile.write(\"\\nEpoch {:03d}:   Loss: {:6.3f},   Accuracy: {:6.3%}\".format(i+1, trainLossArr[i], trainAccArr[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model, write the details on the logFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Dataset Tensor Shape =  (255, 32)\n"
     ]
    }
   ],
   "source": [
    "### Get the Test Dataset\n",
    "\n",
    "# name of dataset with sentences\n",
    "dataName = \"./DataScriber/raw.processed.Test_01.txt\"\n",
    "\n",
    "# from sentences to columns words+punctuation\n",
    "data = loadFile(processingScriber(dataName))\n",
    "\n",
    "### Encode Data and insert target\n",
    "X, y = encodeData(data, tokenizer, punctuationEnc)\n",
    "\n",
    "### Create Sequences With The Target\n",
    "XMod = insertTarget(X, sequenceSize)\n",
    "\n",
    "# one hot encode the labels\n",
    "yMod = tf.one_hot(y, len(punctuationEnc), dtype='int64').numpy()\n",
    "\n",
    "dataBuilt = tf.data.Dataset.from_tensor_slices((XMod, yMod)).batch(batchSize)\n",
    "\n",
    "print(\"\\nTest Dataset Tensor Shape = \", XMod.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFCamembertForMaskedLM.\n",
      "\n",
      "All the weights of TFCamembertForMaskedLM were initialized from the model checkpoint at jplu/tf-camembert-base.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFCamembertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "### build and compile model\n",
    "\n",
    "bert_input = tf.keras.Input(shape=(sequenceSize), dtype='int32', name='bert_input')\n",
    "x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(bert_input)[0]\n",
    "x = tf.keras.layers.Reshape((sequenceSize*vocabSize,))(x)\n",
    "dense_out = tf.keras.layers.Dense(len(punctuationEnc), activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(bert_input, dense_out, name='model')\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.CategoricalCrossentropy(from_logits=False),\n",
    "              metrics=[tf.keras.metrics.Recall(class_id=0, name='Rec_0'),\n",
    "                       tf.keras.metrics.Precision(class_id=0, name='Prec_0'),\n",
    "                       tf.keras.metrics.Recall(class_id=1, name='Rec_1'),\n",
    "                       tf.keras.metrics.Precision(class_id=1, name='Prec_1'),\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsLst = []\n",
    "for r, d, f in os.walk(save_path):\n",
    "    for file in sorted(f):\n",
    "        if \".index\" in file:\n",
    "            modelsLst.append(file[:-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute F1 Score\n",
    "\n",
    "def compF1(rec, pre):\n",
    "    if pre + rec == .0:\n",
    "        return .0\n",
    "    else:\n",
    "        return 2 * (pre*rec) / (pre+rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EVALUATE\n",
      "\n",
      "EVALUATION DATASET TENSOR SHAPE =  (255, 32)\n",
      "ModelsExpScriber/20200819_000417/cp-001.ckpt\n",
      "8/8 [==============================] - 15s 2s/step - loss: 4.7725 - Rec_0: 1.0000 - Prec_0: 0.9725 - Rec_1: 0.0000e+00 - Prec_1: 0.0000e+00\n",
      "F1_0 =  0.986083 - F1_1 =  0.000000\n",
      "ModelsExpScriber/20200819_000417/cp-002.ckpt\n",
      "8/8 [==============================] - 18s 2s/step - loss: 0.4552 - Rec_0: 1.0000 - Prec_0: 0.9764 - Rec_1: 0.1429 - Prec_1: 1.0000\n",
      "F1_0 =  0.988048 - F1_1 =  0.250000\n",
      "ModelsExpScriber/20200819_000417/cp-003.ckpt\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.1117 - Rec_0: 1.0000 - Prec_0: 0.9841 - Rec_1: 0.4286 - Prec_1: 1.0000\n",
      "F1_0 =  0.992000 - F1_1 =  0.600000\n",
      "ModelsExpScriber/20200819_000417/cp-004.ckpt\n",
      "8/8 [==============================] - 18s 2s/step - loss: 0.0461 - Rec_0: 1.0000 - Prec_0: 0.9841 - Rec_1: 0.4286 - Prec_1: 1.0000\n",
      "F1_0 =  0.992000 - F1_1 =  0.600000\n"
     ]
    }
   ],
   "source": [
    "### evaluate models\n",
    "\n",
    "print(\"\\nEVALUATE\")\n",
    "\n",
    "print(\"\\nEVALUATION DATASET TENSOR SHAPE = \", XMod.shape)\n",
    "\n",
    "logFile.write('\\n\\nEVALUATION\\n')\n",
    "for i in range(len(modelsLst)):\n",
    "    checkpointPath = save_path + modelsLst[i]\n",
    "    print(checkpointPath)\n",
    "\n",
    "    # load weights\n",
    "    model.load_weights(checkpointPath)\n",
    "\n",
    "    # evaluate\n",
    "    evaluation = model.evaluate(dataBuilt)\n",
    "    \n",
    "    f1_0 = compF1(evaluation[1],evaluation[2])\n",
    "    f1_1 = compF1(evaluation[3],evaluation[4])\n",
    "    print(\"F1_0 = {:9.6f} - F1_1 = {:9.6f}\".format(f1_0, f1_1))\n",
    "    \n",
    "    # write details on log files\n",
    "    logFile.write(modelsLst[i])\n",
    "    logFile.write(\" - Loss = {:7.4f} - Rec_0 = {:6.4f} - Pre_0 = {:6.4f} - F1_0 = {:9.6f} - Rec_1 = {:6.4f} - Pre_1 = {:6.4f} - F1_1 = {:9.6f}\\n\".format(evaluation[0], evaluation[1], evaluation[2], f1_0, evaluation[3], evaluation[4], f1_1))\n",
    "\n",
    "logFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvExp00",
   "language": "python",
   "name": ".venvexp00"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
