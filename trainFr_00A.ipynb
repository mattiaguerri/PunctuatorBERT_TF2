{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Several Model Versions, With and Without Regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()  # silence TF warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import loadFile\n",
    "from dataProcessing import encodeData, insertTarget, processingScriber, processingOPUS\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFCamembertModel, TFCamembertForMaskedLM\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jplu/tf-camembert-base\", do_lower_case=True)\n",
    "\n",
    "### punctuation encoder\n",
    "punctuationEnc = {\n",
    "    'SPACE': 0,\n",
    "    'PERIOD': 1,\n",
    "}\n",
    "\n",
    "### Set Vocabulary Size\n",
    "vocabSize = 32005\n",
    "\n",
    "### hyper-parameters\n",
    "sequenceSize = 32\n",
    "batchSize = 32\n",
    "learningRate = 1e-5\n",
    "trainLayerIndex = 0\n",
    "numEpo = 3\n",
    "\n",
    "listHyper0 = ['vocabSize', 'sequenceSize', 'batchSize', 'learningRate', 'trainLayerIndex', 'numEpo']\n",
    "listHyper1 = [str(vocabSize), str(sequenceSize), str(batchSize), str(learningRate), str(trainLayerIndex), str(numEpo)]\n",
    "time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_path = 'ModelsExpScriber/{}/'.format(time)\n",
    "os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Data ... \n",
      "\n",
      "Training Dataset Tensor Shape =  (255, 32)\n"
     ]
    }
   ],
   "source": [
    "### Training Dataset\n",
    "print('\\nProcessing Data ... ')\n",
    "\n",
    "# THIS IN CASE STARTING FROM FILE WITH SENTENCES\n",
    "# this is the file structured in sentences\n",
    "trainDataName = \"./DataScriber/raw.processed.Train_01.txt\"\n",
    "# from sentences to columns words+punctuation\n",
    "dataTrain = loadFile(processingScriber(trainDataName))\n",
    "\n",
    "# # THIS IN CASE STARTING FROM FILE WITH COLUMNS\n",
    "# trainDataName = './AudioFeatures/outFile_05.txt'\n",
    "# dataTrain = loadFile(trainDataName)\n",
    "\n",
    "### Encode Data and insert target\n",
    "XTrain, yTrain = encodeData(dataTrain, tokenizer, punctuationEnc)\n",
    "\n",
    "### Create Sequences With The Target\n",
    "XTrainMod = insertTarget(XTrain, sequenceSize)\n",
    "\n",
    "# build the datasets\n",
    "trainDataset = tf.data.Dataset.from_tensor_slices((XTrainMod, yTrain)).batch(batchSize)\n",
    "# trainDataset = tf.data.Dataset.from_tensor_slices((XTrainMod, yTrain)).shuffle(buffer_size=500000).batch(batchSize)\n",
    "\n",
    "print(\"\\nTraining Dataset Tensor Shape = \", XTrainMod.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Baseline Model, No Regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nBulding the Model ... ')\n",
    "\n",
    "# bert_input = tf.keras.Input(shape=(sequenceSize), dtype='int32', name='bert_input')\n",
    "# x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(bert_input)[0]\n",
    "# x = tf.keras.layers.Reshape((sequenceSize*vocabSize,))(x)\n",
    "# dense_out = tf.keras.layers.Dense(len(punctuationEnc))(x)\n",
    "\n",
    "# model = tf.keras.Model(bert_input, dense_out)\n",
    "\n",
    "# # define the loss\n",
    "# loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# def loss(model, x, y):\n",
    "#     y_ = model(x)\n",
    "#     return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "# # func to calculate the gradients\n",
    "# def grad(model, inputs, targets, trainLayerIndex):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         loss_value = loss(model, inputs, targets)\n",
    "#     return loss_value, tape.gradient(loss_value, model.trainable_variables[trainLayerIndex:])\n",
    "\n",
    "# # define the optimizer\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model, Use the LM Head, Input Only One Vector in the Additional Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nBulding the Model ... ')\n",
    "\n",
    "# ind = sequenceSize//2-1  # index of the target vector\n",
    "# bert_input = tf.keras.Input(shape=(sequenceSize), dtype='int32', name='bert_input')\n",
    "# x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(bert_input)[0]\n",
    "# x = x[:, ind, :]\n",
    "# dense_out = tf.keras.layers.Dense(len(punctuationEnc))(x)\n",
    "\n",
    "# model = tf.keras.Model(bert_input, dense_out)\n",
    "\n",
    "# # define the loss\n",
    "# loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# def loss(model, x, y):\n",
    "#     y_ = model(x)\n",
    "#     return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "# # func to calculate the gradients\n",
    "# def grad(model, inputs, targets, trainLayerIndex):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         loss_value = loss(model, inputs, targets)\n",
    "#     return loss_value, tape.gradient(loss_value, model.trainable_variables[trainLayerIndex:])\n",
    "\n",
    "# # define the optimizer\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model Without Using Masked LM Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nBulding the Model ... ')\n",
    "\n",
    "# hiddenDimension = 768\n",
    "\n",
    "# bert_input = tf.keras.Input(shape=(sequenceSize), dtype='int32', name='bert_input')\n",
    "# x = TFCamembertModel.from_pretrained(\"jplu/tf-camembert-base\")(bert_input)[0]\n",
    "# x = tf.keras.layers.Reshape((sequenceSize*hiddenDimension,))(x)\n",
    "# dense_out = tf.keras.layers.Dense(len(punctuationEnc))(x)\n",
    "\n",
    "# model = tf.keras.Model(bert_input, dense_out)\n",
    "\n",
    "# # define the loss\n",
    "# loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# def loss(model, x, y):\n",
    "#     y_ = model(x)\n",
    "#     return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "# # func to calculate the gradients\n",
    "# def grad(model, inputs, targets, trainLayerIndex):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         loss_value = loss(model, inputs, targets)\n",
    "#     return loss_value, tape.gradient(loss_value, model.trainable_variables[trainLayerIndex:])\n",
    "\n",
    "# # define the optimizer\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model Without Using Masked LM Head, Input Only One Vector in the Additional Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bulding the Model ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at jplu/tf-camembert-base were not used when initializing TFCamembertModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFCamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFCamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFCamembertModel were initialized from the model checkpoint at jplu/tf-camembert-base.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFCamembertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "print('\\nBulding the Model ... ')\n",
    "\n",
    "hiddenDimension = 768\n",
    "ind = sequenceSize//2-1  # index of the target vector\n",
    "bert_input = tf.keras.Input(shape=(sequenceSize), dtype='int32', name='bert_input')\n",
    "x = TFCamembertModel.from_pretrained(\"jplu/tf-camembert-base\")(bert_input)[0]\n",
    "# print(type(x))\n",
    "# print(x.shape)\n",
    "x = x[:, ind, :]\n",
    "# print(type(x))\n",
    "# print(x.shape)\n",
    "dense_out = tf.keras.layers.Dense(len(punctuationEnc))(x)\n",
    "\n",
    "model = tf.keras.Model(bert_input, dense_out)\n",
    "\n",
    "# define the loss\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "def loss(model, x, y):\n",
    "    y_ = model(x)\n",
    "    return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "# func to calculate the gradients\n",
    "def grad(model, inputs, targets, trainLayerIndex):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables[trainLayerIndex:])\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model, no regularisation, additional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nBUILD THE MODEL, no regularisation')\n",
    "\n",
    "# bert_input = tf.keras.Input(shape=(sequenceSize), dtype='int32', name='bert_input')\n",
    "# x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(bert_input)[0]\n",
    "# x = tf.keras.layers.Reshape((sequenceSize*vocab_size,))(x)\n",
    "# x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "# dense_out = tf.keras.layers.Dense(2)(x)\n",
    "\n",
    "# model = tf.keras.Model(bert_input, dense_out, name='model')\n",
    "# # print(model.summary())\n",
    "\n",
    "# # define the loss\n",
    "# loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# def loss(model, x, y):\n",
    "#     y_ = model(x)\n",
    "#     return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "# # func to calculate the gradients\n",
    "# def grad(model, inputs, targets):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         loss_value = loss(model, inputs, targets)\n",
    "#     return loss_value, tape.gradient(loss_value, model.trainable_variables[train_layer_ind:])\n",
    "\n",
    "# # define the optimizer\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=learat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Baseline Model, With Regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### get configuration file\n",
    "# modelBERT = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")\n",
    "# configBERT = modelBERT.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configBERT.hidden_dropout_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### change dropout probability\n",
    "# configBERT.hidden_dropout_prob = hidden_dropout_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configBERT.hidden_dropout_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nBUILD THE MODEL, with regularisation')\n",
    "\n",
    "# bert_input = tf.keras.Input(shape=(sequenceSize), dtype='int32', name='bert_input')\n",
    "# x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\", config=configBERT)(bert_input, training=training)[0]\n",
    "# x = tf.keras.layers.Reshape((sequenceSize*vocab_size,))(x)\n",
    "# dense_out = tf.keras.layers.Dense(2)(x)\n",
    "\n",
    "# model = tf.keras.Model(bert_input, dense_out, name='model')\n",
    "# # print(model.summary())\n",
    "\n",
    "# # define the loss\n",
    "# loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# def loss(model, x, y):\n",
    "#     y_ = model(x)\n",
    "#     return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "# # func to calculate the gradients\n",
    "# def grad(model, inputs, targets):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         loss_value = loss(model, inputs, targets)\n",
    "#     return loss_value, tape.gradient(loss_value, model.trainable_variables[train_layer_ind:])\n",
    "\n",
    "# # define the optimizer\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=learat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nSTART TRAINING')\n",
    "\n",
    "# print('\\nX_train.shape = ', X_train.shape)\n",
    "\n",
    "# epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "# epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# epoch_loss_avg_valid = tf.keras.metrics.Mean()\n",
    "# epoch_accuracy_valid = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# train_loss_results = []\n",
    "# train_accuracy_results = []\n",
    "\n",
    "# val_loss_results = []\n",
    "# val_accuracy_results = []\n",
    "\n",
    "# checkpoint_path = save_path + \"cp-{epoch:03d}.ckpt\"\n",
    "\n",
    "# tmpTrain = np.inf\n",
    "# tmpVal = np.inf\n",
    "# for epoch in range(1, (numEpo+1)):\n",
    "\n",
    "#     # Training loop\n",
    "#     for x, y in trainDataset:\n",
    "#         # Optimize the model\n",
    "#         loss_value, grads = grad(model, x, y)\n",
    "#         optimizer.apply_gradients(zip(grads, model.trainable_variables[train_layer_ind:]))\n",
    "\n",
    "#         # Track progress\n",
    "#         epoch_loss_avg.update_state(loss_value)\n",
    "#         epoch_accuracy.update_state(y, model(x))\n",
    "\n",
    "#     # End epoch\n",
    "#     train_loss_results.append(epoch_loss_avg.result())\n",
    "#     train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "#     # if epoch % 10 == 0:\n",
    "#     print(\"\\nEpoch {:03d}: (Training)   Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch, epoch_loss_avg.result(),\n",
    "#                                                                   epoch_accuracy.result()))\n",
    "    \n",
    "#     epoch_loss_avg.reset_states()\n",
    "#     epoch_accuracy.reset_states()\n",
    "    \n",
    "#     # run validation loop\n",
    "#     for x_batch_val, y_batch_val in valDataset:\n",
    "#         loss_value, _ = grad(model, x_batch_val, y_batch_val)\n",
    "#         epoch_loss_avg_valid.update_state(loss_value)\n",
    "#         epoch_accuracy_valid.update_state(y_batch_val, model(x_batch_val))\n",
    "    \n",
    "#     # save model if new min for train loss is found\n",
    "#     if epoch_loss_avg.result().numpy() < tmpTrain:\n",
    "#         tmpTrain = epoch_loss_avg.result().numpy()\n",
    "#         model.save_weights(checkpoint_path.format(epoch=epoch))\n",
    "    \n",
    "# #     # save model if new min for val loss is found\n",
    "# #     if epoch_loss_avg_valid.result().numpy() < tmp:\n",
    "# #         tmp = epoch_loss_avg_valid.result().numpy()\n",
    "# #         model.save_weights(checkpoint_path.format(epoch=epoch))\n",
    "    \n",
    "#     val_loss = epoch_loss_avg_valid.result()\n",
    "#     val_acc = epoch_accuracy_valid.result()\n",
    "#     print(\"           (Validation) Loss: {:.3f}, Accuracy: {:.3%}\".format(val_loss, val_acc))\n",
    "    \n",
    "#     epoch_loss_avg_valid.reset_states()\n",
    "#     epoch_accuracy_valid.reset_states()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop, No Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment Folder:  20200828_114654\n",
      "\n",
      "Hyperparameters:\n",
      "sequenceSize =  32\n",
      "batchSize =  32\n",
      "learningRate =  1e-05\n",
      "train Layer Index =  0\n",
      "numEpo =  3\n",
      "\n",
      "Training the Model ... \n",
      "\n",
      "Epoch 001: (Training)   Loss: 0.436, Accuracy: 97.255%\n",
      "\n",
      "Epoch 002: (Training)   Loss: 0.215, Accuracy: 97.255%\n",
      "\n",
      "Epoch 003: (Training)   Loss: 0.146, Accuracy: 97.255%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExperiment Folder: \", time)\n",
    "print(\"\\nHyperparameters:\")\n",
    "print('sequenceSize = ', sequenceSize)\n",
    "print('batchSize = ', batchSize)\n",
    "print('learningRate = ', learningRate)\n",
    "print('train Layer Index = ', trainLayerIndex)\n",
    "print('numEpo = ', numEpo)\n",
    "\n",
    "epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "checkpoint_path = save_path + \"cp-{epoch:03d}.ckpt\"\n",
    "\n",
    "print(\"\\nTraining the Model ... \")\n",
    "\n",
    "tmpTrain = np.inf\n",
    "for epoch in range(1, numEpo+1):\n",
    "\n",
    "    # training loop\n",
    "    for x, y in trainDataset:\n",
    "        # optimize the model\n",
    "        loss_value, grads = grad(model, x, y, trainLayerIndex)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables[trainLayerIndex:]))\n",
    "\n",
    "        # track progress\n",
    "        epoch_loss_avg.update_state(loss_value)\n",
    "        epoch_accuracy.update_state(y, model(x))\n",
    "\n",
    "    # end epoch\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "    print(\"\\nEpoch {:03d}: (Training)   Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch, epoch_loss_avg.result(), epoch_accuracy.result()))\n",
    "\n",
    "    # # save model if new min for train loss is found\n",
    "    # if epoch_loss_avg.result().numpy() < tmpTrain:\n",
    "    tmpTrain = epoch_loss_avg.result().numpy()\n",
    "    model.save_weights(checkpoint_path.format(epoch=epoch))\n",
    "\n",
    "    epoch_loss_avg.reset_states()\n",
    "    epoch_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Training Details On Log FileÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameLogFile = 'log.txt'\n",
    "logFile = open(save_path + nameLogFile, \"w\")\n",
    "\n",
    "# write name of model\n",
    "logFile.write(\"\\n\" + time + \"\\n\\n\")\n",
    "\n",
    "# write hyper parameters\n",
    "for i in range(len(listHyper0)):\n",
    "    logFile.write(listHyper0[i] + \":  \" + listHyper1[i] + \"\\n\")\n",
    "\n",
    "# write training details\n",
    "logFile.write('\\nTRAINING')\n",
    "trainLossArr = np.asarray(train_loss_results)\n",
    "trainAccArr = np.asarray(train_accuracy_results)\n",
    "for i in range(numEpo):\n",
    "    epoch = i+1\n",
    "    logFile.write(\"\\nEpoch {:03d}:   Loss: {:7.4f},   Accuracy: {:7.4%}\".format(epoch, trainLossArr[i], trainAccArr[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model, Write the Details on the logFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Dataset Tensor Shape =  (255, 32)\n"
     ]
    }
   ],
   "source": [
    "### Get the Test Dataset\n",
    "\n",
    "# name of dataset with sentences\n",
    "dataName = \"./DataScriber/raw.processed.Test_01.txt\"\n",
    "\n",
    "# from sentences to columns words+punctuation\n",
    "data = loadFile(processingScriber(dataName))\n",
    "\n",
    "### Encode Data\n",
    "X, y = encodeData(data, tokenizer, punctuationEnc)\n",
    "\n",
    "### Create Sequences With The Target\n",
    "XMod = insertTarget(X, sequenceSize)\n",
    "\n",
    "# one hot encode the labels\n",
    "yMod = tf.one_hot(y, len(punctuationEnc), dtype='int64').numpy()\n",
    "\n",
    "dataBuilt = tf.data.Dataset.from_tensor_slices((XMod, yMod)).batch(batchSize)\n",
    "\n",
    "print(\"\\nTest Dataset Tensor Shape = \", XMod.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_input = tf.keras.Input(shape=(sequenceSize), dtype='int32', name='bert_input')\n",
    "# x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(bert_input)[0]\n",
    "# x = tf.keras.layers.Reshape((sequenceSize*vocabSize,))(x)\n",
    "# dense_out = tf.keras.layers.Dense(len(punctuationEnc), activation='softmax')(x)\n",
    "# model = tf.keras.Model(bert_input, dense_out, name='model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model, Use the LM Head, Input Only One Vector in the Additional Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind = sequenceSize//2-1  # index of the target vector\n",
    "# bert_input = tf.keras.Input(shape=(sequenceSize), dtype='int32', name='bert_input')\n",
    "# x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(bert_input)[0]\n",
    "# x = x[:, ind, :]\n",
    "# dense_out = tf.keras.layers.Dense(len(punctuationEnc), activation='softmax')(x)\n",
    "# model = tf.keras.Model(bert_input, dense_out, name='model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build CamemBERT, no LM head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiddenDimension = 768\n",
    "# bert_input = tf.keras.Input(shape=(sequenceSize), dtype='int32', name='bert_input')\n",
    "# x = TFCamembertModel.from_pretrained(\"jplu/tf-camembert-base\")(bert_input)[0]\n",
    "# x = tf.keras.layers.Reshape((sequenceSize*hiddenDimension,))(x)\n",
    "# dense_out = tf.keras.layers.Dense(len(punctuationEnc), activation='softmax')(x)\n",
    "# model = tf.keras.Model(bert_input, dense_out, name='model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build CamemBERT, No LM Head, Input Only One Vector in the Additional Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bulding the Model ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at jplu/tf-camembert-base were not used when initializing TFCamembertModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFCamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFCamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFCamembertModel were initialized from the model checkpoint at jplu/tf-camembert-base.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFCamembertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "print('\\nBulding the Model ... ')\n",
    "\n",
    "hiddenDimension = 768\n",
    "ind = sequenceSize//2-1  # index of the target vector\n",
    "bert_input = tf.keras.Input(shape=(sequenceSize), dtype='int32', name='bert_input')\n",
    "x = TFCamembertModel.from_pretrained(\"jplu/tf-camembert-base\")(bert_input)[0]\n",
    "x = x[:, ind, :]\n",
    "dense_out = tf.keras.layers.Dense(len(punctuationEnc), activation='softmax')(x)\n",
    "model = tf.keras.Model(bert_input, dense_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.CategoricalCrossentropy(from_logits=False),\n",
    "              metrics=[tf.keras.metrics.Recall(class_id=0, name='Rec_0'),\n",
    "                       tf.keras.metrics.Precision(class_id=0, name='Prec_0'),\n",
    "                       tf.keras.metrics.Recall(class_id=1, name='Rec_1'),\n",
    "                       tf.keras.metrics.Precision(class_id=1, name='Prec_1'),\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get List of the Models in the Output Folder\n",
    "\n",
    "modelsLst = []\n",
    "for r, d, f in os.walk(save_path):\n",
    "    for file in sorted(f):\n",
    "        if \".index\" in file:\n",
    "            modelsLst.append(file[:-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute F1 Score\n",
    "\n",
    "def compF1(rec, pre):\n",
    "    if pre + rec == .0:\n",
    "        return .0\n",
    "    else:\n",
    "        return 2 * (pre*rec) / (pre+rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate Models\n",
      "ModelsExpScriber/20200828_114654/cp-001.ckpt\n",
      "8/8 [==============================] - 18s 2s/step - loss: 0.2799 - Rec_0: 1.0000 - Prec_0: 0.9725 - Rec_1: 0.0000e+00 - Prec_1: 0.0000e+00\n",
      "F1_0 =  0.9860835 - F1_1 =  0.0000000\n",
      "ModelsExpScriber/20200828_114654/cp-002.ckpt\n",
      "8/8 [==============================] - 19s 2s/step - loss: 0.1680 - Rec_0: 1.0000 - Prec_0: 0.9725 - Rec_1: 0.0000e+00 - Prec_1: 0.0000e+00\n",
      "F1_0 =  0.9860835 - F1_1 =  0.0000000\n",
      "ModelsExpScriber/20200828_114654/cp-003.ckpt\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.1345 - Rec_0: 1.0000 - Prec_0: 0.9725 - Rec_1: 0.0000e+00 - Prec_1: 0.0000e+00\n",
      "F1_0 =  0.9860835 - F1_1 =  0.0000000\n"
     ]
    }
   ],
   "source": [
    "### Evaluate the Models\n",
    "\n",
    "print(\"\\nEvaluate Models\")\n",
    "\n",
    "logFile.write('\\n\\nEVALUATION\\n')\n",
    "for i in range(len(modelsLst)):\n",
    "    checkpointPath = save_path + modelsLst[i]\n",
    "    print(checkpointPath)\n",
    "\n",
    "    # load weights\n",
    "    model.load_weights(checkpointPath)\n",
    "\n",
    "    # evaluate\n",
    "    evaluation = model.evaluate(dataBuilt)\n",
    "    \n",
    "    f1_0 = compF1(evaluation[1],evaluation[2])\n",
    "    f1_1 = compF1(evaluation[3],evaluation[4])\n",
    "    print(\"F1_0 = {:10.7f} - F1_1 = {:10.7f}\".format(f1_0, f1_1))\n",
    "    \n",
    "    # write details on log files\n",
    "    logFile.write(modelsLst[i])\n",
    "    logFile.write(\" - Loss = {:7.4f} - Rec_0 = {:6.4f} - Pre_0 = {:6.4f} - F1_0 = {:10.7f} - Rec_1 = {:6.4f} - Pre_1 = {:6.4f} - F1_1 = {:10.7f}\\n\".format(evaluation[0], evaluation[1], evaluation[2], f1_0, evaluation[3], evaluation[4], f1_1))\n",
    "\n",
    "logFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvExp00",
   "language": "python",
   "name": ".venvexp00"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
