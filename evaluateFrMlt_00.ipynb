{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate The Models Performance, French Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()  # silence TF warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "from dataProcessing import load_file, processingScriber00, encodeData, insert_target\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFCamembertForMaskedLM\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Path To Models\n",
    "# path = \"ModelsExpScriber/20200604_163315/\"\n",
    "path = \"Models/20200530_161559/\"  # Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jplu/tf-camembert-base\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### puntuation encoder\n",
    "punctuation_enc = {\n",
    "    'O': 0,\n",
    "    'PERIOD': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 32005\n",
    "sequenceSize = 32\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21009, 32)\n",
      "(32, 32)\n"
     ]
    }
   ],
   "source": [
    "# name of dataset with sentences\n",
    "data_name = \"Scriber\"\n",
    "fileName = 'Data' + data_name + '/' + 'raw.processed.Test_01.txt'\n",
    "\n",
    "# from sentences to list of words+punctuation\n",
    "data = load_file(processingScriber00(fileName))\n",
    "\n",
    "# encode and insert target\n",
    "X_, y_ = encodeData(data, tokenizer, punctuation_enc)\n",
    "X = insert_target(X_, sequenceSize)\n",
    "y = np.asarray(y_)\n",
    "\n",
    "\n",
    "### get only an n of the data.\n",
    "n = 32\n",
    "print(X.shape)\n",
    "X = X[0:n]\n",
    "y = y[0:n]\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "# one hot encode the labels\n",
    "y = tf.one_hot(y, 2, dtype='int64').numpy()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Get Percentage Of Ones In The Dataset\n",
    "\n",
    "# indTup = np.where(y==1)\n",
    "# ind = indTup[1]\n",
    "# print(np.sum(ind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model, One Additional Layer On Top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BUILD THE MODEL\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('\\nBUILD THE MODEL')\n",
    "\n",
    "bert_input = tf.keras.Input(shape=(sequenceSize), dtype='int32', name='bert_input')\n",
    "x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(bert_input)[0]\n",
    "x = tf.keras.layers.Reshape((sequenceSize*vocab_size,))(x)\n",
    "dense_out = tf.keras.layers.Dense(len(punctuation_enc), activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(bert_input, dense_out, name='model')\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.CategoricalCrossentropy(from_logits=False),\n",
    "              metrics=[tf.keras.metrics.Recall(class_id=0, name='Rec_0'),\n",
    "                       tf.keras.metrics.Precision(class_id=0, name='Prec_0'),\n",
    "                       tf.keras.metrics.Recall(class_id=1, name='Rec_1'),\n",
    "                       tf.keras.metrics.Precision(class_id=1, name='Prec_1'),\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model, Two Additional Layers On The Top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nBUILD THE MODEL')\n",
    "\n",
    "# bert_input = tf.keras.Input(shape=(sequenceSize), dtype='int32', name='bert_input')\n",
    "# x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(bert_input)[0]\n",
    "# x = tf.keras.layers.Reshape((sequenceSize*vocab_size,))(x)\n",
    "# x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "# dense_out = tf.keras.layers.Dense(len(punctuation_enc))(x)\n",
    "\n",
    "# model = tf.keras.Model(bert_input, dense_out, name='model')\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss=tf.losses.CategoricalCrossentropy(from_logits=False),\n",
    "#               metrics=[tf.keras.metrics.Recall(class_id=0, name='Rec_0'),\n",
    "#                        tf.keras.metrics.Precision(class_id=0, name='Prec_0'),\n",
    "#                        tf.keras.metrics.Recall(class_id=1, name='Rec_1'),\n",
    "#                        tf.keras.metrics.Precision(class_id=1, name='Prec_1'),\n",
    "#                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsLst = []\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in sorted(f):\n",
    "        if \".index\" in file:\n",
    "            modelsLst.append(file[:-6])\n",
    "# modelsLst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 * (precision*recall) / (precision+recall)  # formula to compute F1.\n",
    "def compF1(rec, pre):\n",
    "    return 2 * (pre*rec) / (pre+rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (32, 32)\n",
      "Models/20200530_161559/cp-001.ckpt\n",
      "None\n",
      "1/1 [==============================] - 0s 905us/step - loss: 0.1486 - Rec_0: 1.0000 - Prec_0: 0.9355 - Rec_1: 0.3333 - Prec_1: 1.0000\n",
      "F1_0 =   0.9666667     F1_1 =   0.5000000\n",
      "Models/20200530_161559/cp-002.ckpt\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0869 - Rec_0: 1.0000 - Prec_0: 0.9355 - Rec_1: 0.3333 - Prec_1: 1.0000\n",
      "F1_0 =   0.9666667     F1_1 =   0.5000000\n",
      "Models/20200530_161559/cp-003.ckpt\n",
      "1/1 [==============================] - 0s 902us/step - loss: 0.0383 - Rec_0: 1.0000 - Prec_0: 1.0000 - Rec_1: 1.0000 - Prec_1: 1.0000\n",
      "F1_0 =   1.0000000     F1_1 =   1.0000000\n",
      "Models/20200530_161559/cp-004.ckpt\n",
      "1/1 [==============================] - 0s 836us/step - loss: 0.1053 - Rec_0: 1.0000 - Prec_0: 0.9667 - Rec_1: 0.6667 - Prec_1: 1.0000\n",
      "F1_0 =   0.9830508     F1_1 =   0.8000000\n",
      "Models/20200530_161559/cp-005.ckpt\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0805 - Rec_0: 1.0000 - Prec_0: 0.9355 - Rec_1: 0.3333 - Prec_1: 1.0000\n",
      "F1_0 =   0.9666667     F1_1 =   0.5000000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\", X.shape)\n",
    "for i in range(len(modelsLst)):\n",
    "    # get the path\n",
    "    checkpointPath = path + modelsLst[i]\n",
    "    print(checkpointPath)\n",
    "\n",
    "    # load the weights\n",
    "    model.load_weights(checkpointPath)\n",
    "\n",
    "    evaluation = model.evaluate(dataset)\n",
    "    f1_0 = compF1(evaluation[1],evaluation[2])\n",
    "    f1_1 = compF1(evaluation[3],evaluation[4])\n",
    "    print(\"F1_0 = {:11.7f}     F1_1 = {:11.7f}\".format(f1_0, f1_1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_X",
   "language": "python",
   "name": ".venv_x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
