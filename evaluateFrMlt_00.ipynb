{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate The Models Performance, French Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()  # silence TF warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import loadFile\n",
    "from dataProcessing import encodeData, insertTarget, processingScriber, processingOPUS\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFCamembertModel, TFCamembertForMaskedLM\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Path To Models\n",
    "# path = \"ModelsExpScriber/20200604_163315/\"\n",
    "# path = \"Models/20200530_161559/\"  # Baseline Model\n",
    "path = \"ModelsExpScriber/20200827_113149/\"\n",
    "# path = \"ModelsExpScriber/20200826_090912/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jplu/tf-camembert-base\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### puntuation encoder\n",
    "punctuationEnc = {\n",
    "    'SPACE': 0,\n",
    "    'PERIOD': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabSize = 32005\n",
    "sequenceSize = 32\n",
    "batchSize = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Dataset Tensor Shape =  (255, 32)\n"
     ]
    }
   ],
   "source": [
    "# this is the file structured in sentences\n",
    "dataName = \"./DataScriber/raw.processed.Test_01.txt\"\n",
    "\n",
    "# from sentences to columns words+punctuation\n",
    "dataSet = loadFile(processingScriber(dataName))\n",
    "\n",
    "### Encode Data and insert target\n",
    "X, y = encodeData(dataSet, tokenizer, punctuationEnc)\n",
    "\n",
    "### Create Sequences With The Target\n",
    "XMod = insertTarget(X, sequenceSize)\n",
    "\n",
    "# one hot encode the labels\n",
    "yMod = tf.one_hot(y, len(punctuationEnc), dtype='int64').numpy()\n",
    "\n",
    "# build the datasets\n",
    "dataset = tf.data.Dataset.from_tensor_slices((XMod, yMod)).batch(batchSize)\n",
    "\n",
    "print(\"\\nTraining Dataset Tensor Shape = \", XMod.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Get Percentage Of Ones In The Dataset\n",
    "\n",
    "# indTup = np.where(y==1)\n",
    "# ind = indTup[1]\n",
    "# print(np.sum(ind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_input = tf.keras.Input(shape=(sequenceSize), dtype='int32', name='bert_input')\n",
    "x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(bert_input)[0]\n",
    "x = tf.keras.layers.Reshape((sequenceSize*vocab_size,))(x)\n",
    "dense_out = tf.keras.layers.Dense(len(punctuation_enc), activation='softmax')(x)\n",
    "model = tf.keras.Model(bert_input, dense_out, name='model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model, No LM head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at jplu/tf-camembert-base were not used when initializing TFCamembertModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFCamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFCamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFCamembertModel were initialized from the model checkpoint at jplu/tf-camembert-base.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFCamembertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# hiddenDimension = 768\n",
    "# bert_input = tf.keras.Input(shape=(sequenceSize), dtype='int32', name='bert_input')\n",
    "# x = TFCamembertModel.from_pretrained(\"jplu/tf-camembert-base\")(bert_input)[0]\n",
    "# x = tf.keras.layers.Reshape((sequenceSize*hiddenDimension,))(x)\n",
    "# dense_out = tf.keras.layers.Dense(len(punctuationEnc), activation='softmax')(x)\n",
    "# model = tf.keras.Model(bert_input, dense_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model, Use LM head, Plus Two Fully Connwcted Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_input = tf.keras.Input(shape=(sequenceSize), dtype='int32', name='bert_input')\n",
    "# x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(bert_input)[0]\n",
    "# x = tf.keras.layers.Reshape((sequenceSize*vocabSize,))(x)\n",
    "# x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "# dense_out = tf.keras.layers.Dense(len(punctuationEnc))(x)\n",
    "# model = tf.keras.Model(bert_input, dense_out, name='model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.CategoricalCrossentropy(from_logits=False),\n",
    "              metrics=[tf.keras.metrics.Recall(class_id=0, name='Rec_0'),\n",
    "                       tf.keras.metrics.Precision(class_id=0, name='Prec_0'),\n",
    "                       tf.keras.metrics.Recall(class_id=1, name='Rec_1'),\n",
    "                       tf.keras.metrics.Precision(class_id=1, name='Prec_1'),\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get List of the Models in the Output Folder\n",
    "\n",
    "modelsLst = []\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in sorted(f):\n",
    "        if \".index\" in file:\n",
    "            modelsLst.append(file[:-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute F1 Score\n",
    "\n",
    "def compF1(rec, pre):\n",
    "    if pre + rec == .0:\n",
    "        return .0\n",
    "    else:\n",
    "        return 2 * (pre*rec) / (pre+rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelsExpScriber/20200827_113149/cp-001.ckpt\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.3855 - Rec_0: 1.0000 - Prec_0: 0.9725 - Rec_1: 0.0000e+00 - Prec_1: 0.0000e+00\n",
      "F1_0 =   0.9860835     F1_1 =   0.0000000\n",
      "ModelsExpScriber/20200827_113149/cp-002.ckpt\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.1996 - Rec_0: 1.0000 - Prec_0: 0.9725 - Rec_1: 0.0000e+00 - Prec_1: 0.0000e+00\n",
      "F1_0 =   0.9860835     F1_1 =   0.0000000\n",
      "ModelsExpScriber/20200827_113149/cp-003.ckpt\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.1380 - Rec_0: 1.0000 - Prec_0: 0.9725 - Rec_1: 0.0000e+00 - Prec_1: 0.0000e+00\n",
      "F1_0 =   0.9860835     F1_1 =   0.0000000\n"
     ]
    }
   ],
   "source": [
    "### Evaluate the Models\n",
    "\n",
    "for i in range(len(modelsLst)):\n",
    "    # get the path\n",
    "    checkpointPath = path + modelsLst[i]\n",
    "    print(checkpointPath)\n",
    "\n",
    "    # load the weights\n",
    "    model.load_weights(checkpointPath)\n",
    "\n",
    "    evaluation = model.evaluate(dataset)\n",
    "    f1_0 = compF1(evaluation[1],evaluation[2])\n",
    "    f1_1 = compF1(evaluation[3],evaluation[4])\n",
    "    print(\"F1_0 = {:11.7f}     F1_1 = {:11.7f}\".format(f1_0, f1_1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvExp00",
   "language": "python",
   "name": ".venvexp00"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
