{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model With Two Layers On Top Of CamemBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()  # silence TF warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import loadFile\n",
    "from dataProcessing import encodeData, insertTarget, processingScriber, processingOPUS\n",
    "\n",
    "from transformers import AutoTokenizer, TFCamembertForMaskedLM\n",
    "from datetime import datetime\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jplu/tf-camembert-base\", do_lower_case=True)\n",
    "\n",
    "### punctuation encoder\n",
    "punctuationEnc = {\n",
    "    'SPACE': 0,\n",
    "    'PERIOD': 1,\n",
    "}\n",
    "\n",
    "### Set Vocabulary Size\n",
    "vocabSize = 32005\n",
    "\n",
    "### hyper-parameters\n",
    "sequenceSize = 32\n",
    "batchSize = 32\n",
    "learningRate = 1e-5\n",
    "trainLayerIndex = 0\n",
    "numEpo = 3\n",
    "\n",
    "listHyper0 = ['vocabSize', 'sequenceSize', 'batchSize', 'learningRate', 'trainLayerIndex', 'numEpo']\n",
    "listHyper1 = [str(vocabSize), str(sequenceSize), str(batchSize), str(learningRate), str(trainLayerIndex), str(numEpo)]\n",
    "time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_path = 'ModelsExpScriber/{}/'.format(time)\n",
    "os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build The Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Data ... \n",
      "\n",
      "Training Dataset Tensor Shape =  (255, 32)\n"
     ]
    }
   ],
   "source": [
    "### Training Dataset\n",
    "print('\\nProcessing Data ... ')\n",
    "\n",
    "# THIS IN CASE STARTING FROM FILE WITH SENTENCES\n",
    "# this is the file structured in sentences\n",
    "trainDataName = \"./DataScriber/raw.processed.Test_01_extract.txt\"\n",
    "# from sentences to columns words+punctuation\n",
    "dataTrain = loadFile(processingScriber(trainDataName))\n",
    "\n",
    "# # THIS IN CASE STARTING FROM FILE WITH COLUMNS\n",
    "# trainDataName = './AudioFeatures/outFile_05.txt'\n",
    "# dataTrain = loadFile(trainDataName)\n",
    "\n",
    "### Encode Data and insert target\n",
    "XTrain, yTrain = encodeData(dataTrain, tokenizer, punctuationEnc)\n",
    "\n",
    "### Create Sequences With The Target\n",
    "XTrainMod = insertTarget(XTrain, sequenceSize)\n",
    "\n",
    "# build the datasets\n",
    "trainDataset = tf.data.Dataset.from_tensor_slices((XTrainMod, yTrain)).shuffle(buffer_size=500000).batch(batchSize)\n",
    "\n",
    "print(\"\\nTraining Dataset Tensor Shape = \", XTrainMod.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BUILD THE MODEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFCamembertForMaskedLM.\n",
      "\n",
      "All the weights of TFCamembertForMaskedLM were initialized from the model checkpoint at jplu/tf-camembert-base.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFCamembertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "print('\\nBUILD THE MODEL')\n",
    "\n",
    "bert_input = tf.keras.Input(shape=(sequenceSize), dtype='int32', name='bert_input')\n",
    "x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(bert_input)[0]\n",
    "x = tf.keras.layers.Reshape((sequenceSize*vocabSize,))(x)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "dense_out = tf.keras.layers.Dense(len(punctuationEnc))(x)\n",
    "\n",
    "model = tf.keras.Model(bert_input, dense_out)\n",
    "\n",
    "# define the loss\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "def loss(model, x, y):\n",
    "    y_ = model(x)\n",
    "    return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "# func to calculate the gradients\n",
    "def grad(model, inputs, targets, trainLayerIndex):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables[trainLayerIndex:])\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check The Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "208\n"
     ]
    }
   ],
   "source": [
    "# print(len(model.layers))\n",
    "# print(len(model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert_input (InputLayer)      [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "tf_camembert_for_masked_lm_1 ((None, 32, 32005),)      111246085 \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1024160)           0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                65546304  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 176,792,519\n",
      "Trainable params: 176,792,519\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment Folder:  20200819_152002\n",
      "\n",
      "Hyperparameters:\n",
      "sequenceSize =  32\n",
      "batchSize =  32\n",
      "learningRate =  1e-05\n",
      "train Layer Index =  0\n",
      "numEpo =  3\n",
      "\n",
      "Training the Model ... \n",
      "\n",
      "Epoch 001: (Training)   Loss: 13.878, Accuracy: 97.255%\n",
      "\n",
      "Epoch 002: (Training)   Loss: 3.131, Accuracy: 92.549%\n",
      "\n",
      "Epoch 003: (Training)   Loss: 0.301, Accuracy: 97.647%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExperiment Folder: \", time)\n",
    "print(\"\\nHyperparameters:\")\n",
    "print('sequenceSize = ', sequenceSize)\n",
    "print('batchSize = ', batchSize)\n",
    "print('learningRate = ', learningRate)\n",
    "print('train Layer Index = ', trainLayerIndex)\n",
    "print('numEpo = ', numEpo)\n",
    "\n",
    "epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "checkpoint_path = save_path + \"cp-{epoch:03d}.ckpt\"\n",
    "\n",
    "print(\"\\nTraining the Model ... \")\n",
    "\n",
    "tmpTrain = np.inf\n",
    "for epoch in range(1, numEpo+1):\n",
    "\n",
    "    # training loop\n",
    "    for x, y in trainDataset:\n",
    "        # optimize the model\n",
    "        loss_value, grads = grad(model, x, y, trainLayerIndex)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables[trainLayerIndex:]))\n",
    "\n",
    "        # track progress\n",
    "        epoch_loss_avg.update_state(loss_value)\n",
    "        epoch_accuracy.update_state(y, model(x))\n",
    "\n",
    "    # end epoch\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "    print(\"\\nEpoch {:03d}: (Training)   Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch, epoch_loss_avg.result(), epoch_accuracy.result()))\n",
    "\n",
    "    # # save model if new min for train loss is found\n",
    "    # if epoch_loss_avg.result().numpy() < tmpTrain:\n",
    "    tmpTrain = epoch_loss_avg.result().numpy()\n",
    "    model.save_weights(checkpoint_path.format(epoch=epoch))\n",
    "\n",
    "    epoch_loss_avg.reset_states()\n",
    "    epoch_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Training Details On Log File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameLogFile = 'log.txt'\n",
    "logFile = open(save_path + nameLogFile, \"w\")\n",
    "\n",
    "# write name of model\n",
    "logFile.write(\"\\n\" + time + \"\\n\\n\")\n",
    "\n",
    "# write hyper parameters\n",
    "for i in range(len(listHyper0)):\n",
    "    logFile.write(listHyper0[i] + \":  \" + listHyper1[i] + \"\\n\")\n",
    "\n",
    "# write training details\n",
    "logFile.write('\\nTRAINING')\n",
    "trainLossArr = np.asarray(train_loss_results)\n",
    "trainAccArr = np.asarray(train_accuracy_results)\n",
    "for i in range(numEpo):\n",
    "    epoch = i+1\n",
    "    logFile.write(\"\\nEpoch {:03d}:   Loss: {:7.4f},   Accuracy: {:7.4%}\".format(epoch, trainLossArr[i], trainAccArr[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model, write the details on the logFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Dataset Tensor Shape =  (255, 32)\n"
     ]
    }
   ],
   "source": [
    "### Get the Test Dataset\n",
    "\n",
    "# name of dataset with sentences\n",
    "dataName = \"./DataScriber/raw.processed.Test_01_extract.txt\"\n",
    "\n",
    "# from sentences to columns words+punctuation\n",
    "data = loadFile(processingScriber(dataName))\n",
    "\n",
    "### Encode Data and insert target\n",
    "X, y = encodeData(data, tokenizer, punctuationEnc)\n",
    "\n",
    "### Create Sequences With The Target\n",
    "XMod = insertTarget(X, sequenceSize)\n",
    "\n",
    "# one hot encode the labels\n",
    "yMod = tf.one_hot(y, len(punctuationEnc), dtype='int64').numpy()\n",
    "\n",
    "dataBuilt = tf.data.Dataset.from_tensor_slices((XMod, yMod)).batch(batchSize)\n",
    "\n",
    "print(\"\\nTest Dataset Tensor Shape = \", XMod.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFCamembertForMaskedLM.\n",
      "\n",
      "All the weights of TFCamembertForMaskedLM were initialized from the model checkpoint at jplu/tf-camembert-base.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFCamembertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "### build and compile model\n",
    "\n",
    "bert_input = tf.keras.Input(shape=(sequenceSize), dtype='int32', name='bert_input')\n",
    "x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(bert_input)[0]\n",
    "x = tf.keras.layers.Reshape((sequenceSize*vocabSize,))(x)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "dense_out = tf.keras.layers.Dense(len(punctuationEnc), activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(bert_input, dense_out, name='model')\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.CategoricalCrossentropy(from_logits=False),\n",
    "              metrics=[tf.keras.metrics.Recall(class_id=0, name='Rec_0'),\n",
    "                       tf.keras.metrics.Precision(class_id=0, name='Prec_0'),\n",
    "                       tf.keras.metrics.Recall(class_id=1, name='Rec_1'),\n",
    "                       tf.keras.metrics.Precision(class_id=1, name='Prec_1'),\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get List of the Models in the Output Folder\n",
    "\n",
    "modelsLst = []\n",
    "for r, d, f in os.walk(save_path):\n",
    "    for file in sorted(f):\n",
    "        if \".index\" in file:\n",
    "            modelsLst.append(file[:-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute F1 Score\n",
    "\n",
    "def compF1(rec, pre):\n",
    "    if pre + rec == .0:\n",
    "        return .0\n",
    "    else:\n",
    "        return 2 * (pre*rec) / (pre+rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate Models\n",
      "ModelsExpScriber/20200819_152002/cp-001.ckpt\n",
      "8/8 [==============================] - 15s 2s/step - loss: 7.9783 - Rec_0: 1.0000 - Prec_0: 0.9725 - Rec_1: 0.0000e+00 - Prec_1: 0.0000e+00\n",
      "F1_0 =  0.9860835 - F1_1 =  0.0000000\n",
      "ModelsExpScriber/20200819_152002/cp-002.ckpt\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.0771 - Rec_0: 0.9677 - Prec_0: 1.0000 - Rec_1: 1.0000 - Prec_1: 0.4667\n",
      "F1_0 =  0.9836065 - F1_1 =  0.6363636\n",
      "ModelsExpScriber/20200819_152002/cp-003.ckpt\n",
      "8/8 [==============================] - 18s 2s/step - loss: 0.3408 - Rec_0: 1.0000 - Prec_0: 0.9725 - Rec_1: 0.0000e+00 - Prec_1: 0.0000e+00\n",
      "F1_0 =  0.9860835 - F1_1 =  0.0000000\n"
     ]
    }
   ],
   "source": [
    "### Evaluate the Models\n",
    "\n",
    "print(\"\\nEvaluate Models\")\n",
    "\n",
    "logFile.write('\\n\\nEVALUATION\\n')\n",
    "for i in range(len(modelsLst)):\n",
    "    checkpointPath = save_path + modelsLst[i]\n",
    "    print(checkpointPath)\n",
    "\n",
    "    # load weights\n",
    "    model.load_weights(checkpointPath)\n",
    "\n",
    "    # evaluate\n",
    "    evaluation = model.evaluate(dataBuilt)\n",
    "    \n",
    "    f1_0 = compF1(evaluation[1],evaluation[2])\n",
    "    f1_1 = compF1(evaluation[3],evaluation[4])\n",
    "    print(\"F1_0 = {:10.7f} - F1_1 = {:10.7f}\".format(f1_0, f1_1))\n",
    "    \n",
    "    # write details on log files\n",
    "    logFile.write(modelsLst[i])\n",
    "    logFile.write(\" - Loss = {:7.4f} - Rec_0 = {:6.4f} - Pre_0 = {:6.4f} - F1_0 = {:10.7f} - Rec_1 = {:6.4f} - Pre_1 = {:6.4f} - F1_1 = {:10.7f}\\n\".format(evaluation[0], evaluation[1], evaluation[2], f1_0, evaluation[3], evaluation[4], f1_1))\n",
    "\n",
    "logFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvExp00",
   "language": "python",
   "name": ".venvexp00"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
