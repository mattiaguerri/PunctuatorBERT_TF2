{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model With One Layer on Top of CamemBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()  # silence TF warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import loadFile\n",
    "from dataProcessing import encodeData, insertTarget, processingScriber\n",
    "from transformers import AutoTokenizer, TFCamembertForMaskedLM\n",
    "from datetime import datetime\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jplu/tf-camembert-base\", do_lower_case=True)\n",
    "\n",
    "### punctuation encoder\n",
    "puncEncoder = {\n",
    "    'SPACE': 0,\n",
    "    'PERIOD': 1,\n",
    "}\n",
    "\n",
    "### Set Vocabulary Size\n",
    "vocabSize = 32005\n",
    "\n",
    "### hyper-parameters\n",
    "sequenceSize = 32\n",
    "batchSize = 32\n",
    "learningRate = 1e-5\n",
    "trainLayerIndex = 0\n",
    "numEpo = 3\n",
    "\n",
    "listHyper0 = ['vocabSize', 'sequenceSize', 'batchSize', 'learningRate', 'trainLayerIndex', 'numEpo']\n",
    "listHyper1 = [str(vocabSize), str(sequenceSize), str(batchSize), str(learningRate), str(trainLayerIndex), str(numEpo)]\n",
    "time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_path = 'ModelsExpTimeStamps/{}/'.format(time)\n",
    "os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Data ... \n",
      "\n",
      "Training Dataset Tensor Shape =  (3200, 32)\n"
     ]
    }
   ],
   "source": [
    "### Training Dataset\n",
    "print('\\nProcessing Data ... ')\n",
    "\n",
    "# # THIS IN CASE STARTING FROM FILE WITH SENTENCES\n",
    "# # this is the file structured in sentences\n",
    "# trainDataName = './outFile_030.txt'  # file path + name\n",
    "# # from sentences to columns words+punctuation\n",
    "# dataTrain = loadFile(processingScriber(trainDataName))\n",
    "\n",
    "# THIS IN CASE STARTING FROM FILE WITH COLUMNS\n",
    "trainDataName = './AudioFeatures/syntheticTrainSet_3200.txt'\n",
    "dataTrain = loadFile(trainDataName)\n",
    "\n",
    "# encode data and insert target\n",
    "XTrain, yTrain = encodeData(dataTrain, tokenizer, puncEncoder)\n",
    "XTrainMod = insertTarget(XTrain, sequenceSize)\n",
    "yTrain = np.asarray(yTrain)\n",
    "\n",
    "# ### Get Only A Fraction Of Data\n",
    "# n = 320\n",
    "# XTrainMod = XTrainMod[0:n]\n",
    "# yTrain = yTrain[0:n]\n",
    "\n",
    "# build the datasets\n",
    "# trainDataset = tf.data.Dataset.from_tensor_slices((XTrainMod, yTrain)).batch(batchSize)\n",
    "trainDataset = tf.data.Dataset.from_tensor_slices((XTrainMod, yTrain)).shuffle(buffer_size=1000000).batch(batchSize)\n",
    "\n",
    "print(\"\\nTraining Dataset Tensor Shape = \", XTrainMod.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bulding the Model ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFCamembertForMaskedLM.\n",
      "\n",
      "All the weights of TFCamembertForMaskedLM were initialized from the model checkpoint at jplu/tf-camembert-base.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFCamembertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "print('\\nBulding the Model ... ')\n",
    "\n",
    "bert_input = tf.keras.Input(shape=(sequenceSize), dtype='int32', name='bert_input')\n",
    "x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(bert_input)[0]\n",
    "x = tf.keras.layers.Reshape((sequenceSize*vocabSize,))(x)\n",
    "dense_out = tf.keras.layers.Dense(len(puncEncoder))(x)\n",
    "\n",
    "model = tf.keras.Model(bert_input, dense_out)\n",
    "\n",
    "# define the loss\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "def loss(model, x, y):\n",
    "    y_ = model(x)\n",
    "    return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "# func to calculate the gradients\n",
    "def grad(model, inputs, targets, trainLayerIndex):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables[trainLayerIndex:])\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Weights From A Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpointPath = \"/ModelsExpScriber/20200628_151812/cp-001.ckpt\"\n",
    "# model.load_weights(checkpointPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study The Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(model.layers))\n",
    "# print(len(model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print word_embeddings Weights\n",
    "# print(model.trainable_variables[194][0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Load Weigths From a Previous Experiment\n",
    "\n",
    "# checkpointPath = \"./Models/20200530_161559/cp-001.ckpt\"  # this is the baseline model\n",
    "# model.load_weights(checkpointPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print word_embeddings Weights\n",
    "# print(model.trainable_variables[194][0:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eperiment Folder:  20200722_174758\n",
      "\n",
      "Hyperparameters:\n",
      "sequenceSize =  32\n",
      "batchSize =  32\n",
      "learningRate =  1e-05\n",
      "train Layer Index =  0\n",
      "numEpo =  3\n",
      "\n",
      "Training the Model ... \n",
      "\n",
      "Epoch 001: (Training)   Loss: 3.262, Accuracy: 73.906%\n",
      "\n",
      "Epoch 002: (Training)   Loss: 0.665, Accuracy: 75.844%\n",
      "\n",
      "Epoch 003: (Training)   Loss: 0.712, Accuracy: 76.094%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEperiment Folder: \", time)\n",
    "print(\"\\nHyperparameters:\")\n",
    "print('sequenceSize = ', sequenceSize)\n",
    "print('batchSize = ', batchSize)\n",
    "print('learningRate = ', learningRate)\n",
    "print('train Layer Index = ', trainLayerIndex)\n",
    "print('numEpo = ', numEpo)\n",
    "\n",
    "epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "checkpoint_path = save_path + \"cp-{epoch:03d}.ckpt\"\n",
    "\n",
    "print(\"\\nTraining the Model ... \")\n",
    "\n",
    "tmpTrain = np.inf\n",
    "for epoch in range(1, numEpo+1):\n",
    "\n",
    "    # training loop\n",
    "    for x, y in trainDataset:\n",
    "        # optimize the model\n",
    "        loss_value, grads = grad(model, x, y, trainLayerIndex)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables[trainLayerIndex:]))\n",
    "\n",
    "        # track progress\n",
    "        epoch_loss_avg.update_state(loss_value)\n",
    "        epoch_accuracy.update_state(y, model(x))\n",
    "\n",
    "    # end epoch\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "    print(\"\\nEpoch {:03d}: (Training)   Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch, epoch_loss_avg.result(), epoch_accuracy.result()))\n",
    "\n",
    "    # # save model if new min for train loss is found\n",
    "    # if epoch_loss_avg.result().numpy() < tmpTrain:\n",
    "    tmpTrain = epoch_loss_avg.result().numpy()\n",
    "    model.save_weights(checkpoint_path.format(epoch=epoch))\n",
    "\n",
    "    epoch_loss_avg.reset_states()\n",
    "    epoch_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Training Details On Log File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nameLogFile = 'log.txt'\n",
    "# logFile = open(save_path + nameLogFile, \"w\")\n",
    "\n",
    "# # write name of model\n",
    "# logFile.write(\"\\n\" + time + \"\\n\\n\")\n",
    "\n",
    "# # write hyper parameters\n",
    "# for i in range(len(listHyper0)):\n",
    "#     logFile.write(listHyper0[i] + \":  \" + listHyper1[i] + \"\\n\")\n",
    "\n",
    "# # write training details\n",
    "# logFile.write('\\nTRAINING')\n",
    "# trainLossArr = np.asarray(train_loss_results)\n",
    "# trainAccArr = np.asarray(train_accuracy_results)\n",
    "# for i in range(numEpo):\n",
    "#     epoch = i+1\n",
    "#     logFile.write(\"\\nEpoch {:03d}:   Loss: {:7.4f},   Accuracy: {:7.4%}\".format(epoch, trainLossArr[i], trainAccArr[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model, write the details on the logFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Get the Test Dataset\n",
    "\n",
    "# # name of dataset with sentences\n",
    "# data_name = \"Scriber\"\n",
    "# fileName = 'Data' + data_name + '/' + 'raw.processed.Test_01.txt'\n",
    "\n",
    "# # from sentences to list of words+punctuation\n",
    "# data = load_file(processingScriber00(fileName))\n",
    "\n",
    "# # encode and insert target\n",
    "# X_, y_ = encodeData(data, tokenizer, puncEncoder)\n",
    "# X = insert_target(X_, sequenceSize)\n",
    "# y = np.asarray(y_)\n",
    "\n",
    "# # get only an n of the data.\n",
    "# n = 32\n",
    "# print(X.shape)\n",
    "# X = X[0:n]\n",
    "# y = y[0:n]\n",
    "# print(X.shape)\n",
    "\n",
    "# # one hot encode the labels\n",
    "# y = tf.one_hot(y, 2, dtype='int64').numpy()\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((X, y)).batch(batchSize)\n",
    "\n",
    "# print(\"\\nTest Dataset Tensor Shape = \", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### build and compile model\n",
    "\n",
    "# bert_input = tf.keras.Input(shape=(sequenceSize), dtype='int32', name='bert_input')\n",
    "# x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(bert_input)[0]\n",
    "# x = tf.keras.layers.Reshape((sequenceSize*vocabSize,))(x)\n",
    "# dense_out = tf.keras.layers.Dense(len(puncEncoder), activation='softmax')(x)\n",
    "\n",
    "# model = tf.keras.Model(bert_input, dense_out, name='model')\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss=tf.losses.CategoricalCrossentropy(from_logits=False),\n",
    "#               metrics=[tf.keras.metrics.Recall(class_id=0, name='Rec_0'),\n",
    "#                        tf.keras.metrics.Precision(class_id=0, name='Prec_0'),\n",
    "#                        tf.keras.metrics.Recall(class_id=1, name='Rec_1'),\n",
    "#                        tf.keras.metrics.Precision(class_id=1, name='Prec_1'),\n",
    "#                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Get List of the Models in the Output Folder\n",
    "\n",
    "# modelsLst = []\n",
    "# for r, d, f in os.walk(save_path):\n",
    "#     for file in sorted(f):\n",
    "#         if \".index\" in file:\n",
    "#             modelsLst.append(file[:-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Compute F1 Score\n",
    "\n",
    "# def compF1(rec, pre):\n",
    "#     if pre + rec == .0:\n",
    "#         return .0\n",
    "#     else:\n",
    "#         return 2 * (pre*rec) / (pre+rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Evaluate the Models\n",
    "\n",
    "# print(\"\\nEvaluate Models\")\n",
    "\n",
    "# print(\"\\nTest Set Tensor Shape = \", X.shape)\n",
    "\n",
    "# logFile.write('\\n\\nEVALUATION\\n')\n",
    "# for i in range(len(modelsLst)):\n",
    "#     checkpointPath = save_path + modelsLst[i]\n",
    "#     print(checkpointPath)\n",
    "\n",
    "#     # load weights\n",
    "#     model.load_weights(checkpointPath)\n",
    "\n",
    "#     # evaluate\n",
    "#     evaluation = model.evaluate(dataset)\n",
    "    \n",
    "#     f1_0 = compF1(evaluation[1],evaluation[2])\n",
    "#     f1_1 = compF1(evaluation[3],evaluation[4])\n",
    "#     print(\"F1_0 = {:10.7f} - F1_1 = {:10.7f}\".format(f1_0, f1_1))\n",
    "    \n",
    "#     # write details on log files\n",
    "#     logFile.write(modelsLst[i])\n",
    "#     logFile.write(\" - Loss = {:7.4f} - Rec_0 = {:6.4f} - Pre_0 = {:6.4f} - F1_0 = {:10.7f} - Rec_1 = {:6.4f} - Pre_1 = {:6.4f} - F1_1 = {:10.7f}\\n\".format(evaluation[0], evaluation[1], evaluation[2], f1_0, evaluation[3], evaluation[4], f1_1))\n",
    "\n",
    "# logFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvExp00",
   "language": "python",
   "name": ".venvexp00"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
