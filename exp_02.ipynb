{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Positional Encoding In The Experimantal Model [Script 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional encoding from the script position_encoding_01.ipynb.  \n",
    "Experimental model from the script trainFr_041A.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()  # silence TF warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "from dataProcessing import load_file, insertTarget\n",
    "from transformers import AutoTokenizer, TFCamembertForMaskedLM\n",
    "from datetime import datetime\n",
    "import copy\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jplu/tf-camembert-base\", do_lower_case=True)\n",
    "\n",
    "### punctuation encoder\n",
    "punctuation_enc = {\n",
    "    'SPACE': 0,\n",
    "    'PERIOD': 1,\n",
    "}\n",
    "outputDimension = len(punctuation_enc)\n",
    "\n",
    "### Set Vocabulary Size, Also Specify The BERT Hidden Dimension\n",
    "vocabSize = 32005\n",
    "hiddenDimension = 768\n",
    "\n",
    "### hyper-parameters\n",
    "batchSize = 6\n",
    "sequenceSize = 32\n",
    "learningRate = 1e-5\n",
    "trainLayerIndex = 0\n",
    "numEpo = 7\n",
    "\n",
    "listHyperNames = ['sequenceSize', 'batchSize', 'learningRate', 'trainLayerIndex', 'numEpo']\n",
    "listHyperValues = [str(sequenceSize), str(batchSize), str(learningRate), str(trainLayerIndex), str(numEpo)]\n",
    "time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_path = 'ModelsExpScriber/{}/'.format(time)\n",
    "# os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeDataTimeStamp(data, tokenizer, punctuation_enc):\n",
    "    XTokensIds = []\n",
    "    XTokensIdsBeg = []  \n",
    "    XTokensIdsEnd = []  \n",
    "    Y = []\n",
    "    count = -1\n",
    "    for line in data:\n",
    "        count += 1\n",
    "        word, punc, wordBeg, wordEnd = line.split(\"\\t\")\n",
    "        tokens = tokenizer.tokenize(word)\n",
    "        tokensIds = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        if len(tokensIds) > 0:\n",
    "            ### note that one word can be encoded in more than one token\n",
    "            if len(tokensIds) > 1:\n",
    "                y = (len(tokensIds)-1) * [0]\n",
    "                numTokens = len(tokensIds)\n",
    "                for i in range(numTokens-1):\n",
    "                    XTokensIdsBeg.append(float(wordBeg))\n",
    "                    XTokensIdsEnd.append(float(wordEnd))\n",
    "                Y += y\n",
    "                # print(\"Line Index = \", count+1)\n",
    "            XTokensIds += tokensIds\n",
    "            XTokensIdsBeg.append(float(wordBeg))\n",
    "            XTokensIdsEnd.append(float(wordEnd))\n",
    "            Y += [punctuation_enc[punc]]\n",
    "    return XTokensIds, XTokensIdsBeg, XTokensIdsEnd, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctTimeStamps(sequenceBegins, sequenceEnds, sequenceSize):\n",
    "    \"\"\"\n",
    "    Apply two corrections to the time stamps:\n",
    "        . wordEnd always larger than nextWordBegin.\n",
    "        . Start time for the sequnce is zero.\n",
    "    \"\"\"\n",
    "\n",
    "    ### CORRECTION 1\n",
    "    ### Apply the correction to the time stamps.\n",
    "    sequenceBeginsCorr = np.asarray(copy.deepcopy(sequenceBegins))\n",
    "    sequenceEndsCorr = np.asarray(copy.deepcopy(sequenceEnds))\n",
    "    for i in range(sequenceSize-1):\n",
    "        wordBegin = sequenceBegins[i]\n",
    "        wordEnd = sequenceEnds[i]\n",
    "        nextWordBegin = sequenceBegins[i+1]\n",
    "        nextWordEnd = sequenceEnds[i+1]\n",
    "        ### i add an additional condition because sometimes wordEnd > nextWordBegin\n",
    "        ### but not beacause of the start of a new sentence.\n",
    "        if wordBegin != nextWordBegin and wordEnd != nextWordEnd:\n",
    "            if wordEnd > nextWordBegin and abs(wordEnd - nextWordBegin) > 0.021:\n",
    "                sequenceBeginsCorr[i+1:] += wordEnd\n",
    "                sequenceEndsCorr[i+1:] += wordEnd\n",
    "#         ### same as before but without the additional condition\n",
    "#         if wordEnd > nextWordBegin:\n",
    "#             sequenceBeginsCorr[i+1:] += wordEnd\n",
    "#             sequenceEndsCorr[i+1:] += wordEnd\n",
    "    ### CORRECTION 2\n",
    "    ### Set beginning of first word in the sentence as time zero.\n",
    "    sequenceBeginsCorr[:] -= sequenceBegins[0]\n",
    "    sequenceEndsCorr[:] -= sequenceBegins[0]\n",
    "\n",
    "    return list(sequenceBeginsCorr), list(sequenceEndsCorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertTargetTimeStamps(x, xBeg, xEnd, sequenceSize):\n",
    "    \n",
    "    X = []\n",
    "    XBeg = []\n",
    "    XEnd = []\n",
    "    x_pad = x[-((sequenceSize-1)//2-1):]+x+x[:sequenceSize//2]\n",
    "    xBeg_pad = xBeg[-((sequenceSize-1)//2-1):]+xBeg+xBeg[:sequenceSize//2]\n",
    "    xEndPad = xEnd[-((sequenceSize-1)//2-1):]+xEnd+xEnd[:sequenceSize//2]\n",
    "\n",
    "    for i in range(len(x_pad)-sequenceSize+2):\n",
    "    # for i in range(1):\n",
    "\n",
    "        ind = (sequenceSize-1)//2\n",
    "\n",
    "        sequence = x_pad[i:i+sequenceSize-1]\n",
    "        sequence.insert(ind, 0)\n",
    "        X.append(sequence)\n",
    "\n",
    "        sequenceBegs = xBeg_pad[i:i+sequenceSize-1]\n",
    "        sequenceEnds = xEndPad[i:i+sequenceSize-1]\n",
    "        \n",
    "        # Apply corrections to the timestamps.\n",
    "        sequenceBegsCorr, sequenceEndsCorr = correctTimeStamps(sequenceBegs, sequenceEnds, len(sequenceEnds))\n",
    "        \n",
    "        val = sequenceBegsCorr[ind-1]\n",
    "        sequenceBegsCorr.insert(ind, val)\n",
    "        \n",
    "        val = sequenceEndsCorr[ind-1]\n",
    "        sequenceEndsCorr.insert(ind, val)\n",
    "\n",
    "        # Collect corrected data.\n",
    "        XBeg.append(sequenceBegsCorr)\n",
    "        XEnd.append(sequenceEndsCorr)\n",
    "\n",
    "    return np.array(X), np.array(XBeg), np.array(XEnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Training Data ... \n",
      "\n",
      "Training Dataset Tensor Shape =  (2343548, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "### Get Training Dataset\n",
    "print('\\nProcessing Training Data ... ')\n",
    "\n",
    "# load the file\n",
    "trainDataName = 'outFile_03.txt'\n",
    "dataTrain = load_file(trainDataName)\n",
    "\n",
    "# encode data and insert target\n",
    "XTrain, XTrainBeg, XTrainEnd, yTrain = encodeDataTimeStamp(dataTrain, tokenizer, punctuation_enc)\n",
    "XTrainMod, XTrainBegMod, XTrainEndMod = insertTargetTimeStamps(XTrain, XTrainBeg, XTrainEnd, sequenceSize)\n",
    "yTrain = np.asarray(yTrain)\n",
    "\n",
    "XTrainAll = np.stack((XTrainMod, XTrainBegMod, XTrainEndMod), axis = 2)\n",
    "\n",
    "### Build The Dataset\n",
    "trainDataset = tf.data.Dataset.from_tensor_slices((XTrainAll, yTrain)).shuffle(buffer_size=500000).batch(batchSize)\n",
    "\n",
    "print(\"\\nTraining Dataset Tensor Shape = \", XTrainAll.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define The Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positionalEncoding(sequence, depth):\n",
    "    \n",
    "    batchSize = sequence.shape[0]\n",
    "    sequenceSize = sequence.shape[1]\n",
    "    \n",
    "    min_rate = 1/10000\n",
    "\n",
    "    assert depth%2 == 0, \"Depth must be even.\"\n",
    "    angle_rate_exponents = np.linspace(0,1,depth//2)\n",
    "    angle_rates = min_rate**(angle_rate_exponents)\n",
    "    \n",
    "    angle_rads = sequence[:, :, np.newaxis]*angle_rates[np.newaxis, np.newaxis, :]\n",
    "\n",
    "    out = np.empty((batchSize, sequenceSize, depth))\n",
    "    for i in range(batchSize):\n",
    "        sines = np.sin(angle_rads[i, :, :])\n",
    "        cosines = np.cos(angle_rads[i, :, :])\n",
    "        arr = np.reshape(np.vstack((sines, cosines)).ravel('F'), (sequenceSize, depth), order='F')\n",
    "        out[i, :, :] = arr\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build The Experimental Model And Test It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bulding the Model ... \n"
     ]
    }
   ],
   "source": [
    "### Build The Experimental Model\n",
    "print('\\nBulding the Model ... ')\n",
    "\n",
    "inpA = tf.keras.Input(shape=(sequenceSize), dtype='int32')\n",
    "inpB = tf.keras.Input(shape=(sequenceSize, hiddenDimension), batch_size=batchSize, dtype='float32')\n",
    "x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(inpA, custom_embeds=inpB)[0]\n",
    "x = tf.keras.layers.Reshape((sequenceSize*vocabSize,))(x)\n",
    "out = tf.keras.layers.Dense(len(punctuation_enc))(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[inpA, inpB], outputs=[out])\n",
    "\n",
    "# define the loss\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "def loss(model, x, custom_embeds, y):\n",
    "    y_ = model([x, custom_embeds])\n",
    "    return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "# func to calculate the gradients\n",
    "def grad(model, inputs, custom_embeds, targets, trainLayerIndex):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, custom_embeds, targets)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables[trainLayerIndex:])\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment Folder:  20200711_192539\n",
      "\n",
      "Hyperparameters:\n",
      "vocabSize =  32005\n",
      "sequenceSize =  32\n",
      "batchSize =  6\n",
      "leaRat =  1e-05\n",
      "Train Layer Index =  0\n",
      "numEpo =  7\n",
      "\n",
      "Training the Model ... \n",
      "\n",
      "Epoch 001: (Training)   Loss: 2.532, Accuracy: 87.660%\n",
      "\n",
      "Epoch 002: (Training)   Loss: 1.275, Accuracy: 89.744%\n",
      "\n",
      "Epoch 003: (Training)   Loss: 0.540, Accuracy: 88.622%\n",
      "\n",
      "Epoch 004: (Training)   Loss: 0.519, Accuracy: 91.506%\n",
      "\n",
      "Epoch 005: (Training)   Loss: 0.349, Accuracy: 91.346%\n",
      "\n",
      "Epoch 006: (Training)   Loss: 0.393, Accuracy: 92.468%\n",
      "\n",
      "Epoch 007: (Training)   Loss: 0.391, Accuracy: 91.186%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExperiment Folder: \", time)\n",
    "print(\"\\nHyperparameters:\")\n",
    "print('vocabSize = ', vocabSize)\n",
    "print('sequenceSize = ', sequenceSize)\n",
    "print('batchSize = ', batchSize)\n",
    "print('leaRat = ', learningRate)\n",
    "print('Train Layer Index = ', trainLayerIndex)\n",
    "print('numEpo = ', numEpo)\n",
    "\n",
    "epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "checkpoint_path = save_path + \"cp-{epoch:03d}.ckpt\"\n",
    "\n",
    "print(\"\\nTraining the Model ... \")\n",
    "for epoch in range(1, numEpo+1):\n",
    "\n",
    "    # training loop\n",
    "    for x, y in trainDataset:\n",
    "        \n",
    "        tokensTensor = tf.cast(x[:, :, 0], dtype=\"int64\")\n",
    "        beginsTensor = x[:, :, 1]\n",
    "        endsTensor = x[:, :, 2]\n",
    "\n",
    "        # get positional encoding tensor for time stamps\n",
    "        depth = hiddenDimension//2\n",
    "        inputBeginsTensor = positionalEncoding(beginsTensor, depth)\n",
    "        inputEndsTensor = positionalEncoding(endsTensor, depth)\n",
    "        inputPosTensor = tf.convert_to_tensor(np.concatenate((inputBeginsTensor, inputEndsTensor), axis=2))\n",
    "\n",
    "        # optimize the model\n",
    "        loss_value, grads = grad(model, tokensTensor, inputPosTensor, y, trainLayerIndex)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables[trainLayerIndex:]))\n",
    "\n",
    "        # track progress\n",
    "        epoch_loss_avg.update_state(loss_value)\n",
    "        epoch_accuracy.update_state(y, model([tokensTensor, inputPosTensor]))\n",
    "\n",
    "    # end epoch\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "    print(\"\\nEpoch {:03d}: (Training)   Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch, epoch_loss_avg.result(), epoch_accuracy.result()))\n",
    "\n",
    "    # # save model if new min for train loss is found\n",
    "    tmpTrain = epoch_loss_avg.result().numpy()\n",
    "    model.save_weights(checkpoint_path.format(epoch=epoch))\n",
    "\n",
    "    epoch_loss_avg.reset_states()\n",
    "    epoch_accuracy.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvTrans",
   "language": "python",
   "name": "venvtrans"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
