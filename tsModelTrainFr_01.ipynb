{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train The Time Stamps Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input file is obtained with the script processAlignedTranscripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()  # silence TF warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import loadFile, encodeDataTimeStamps, correctTimeStamps, insertTargetTimeStamps, positionalEncoding\n",
    "from transformers import AutoTokenizer, TFCamembertForMaskedLM\n",
    "from datetime import datetime\n",
    "import copy\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jplu/tf-camembert-base\", do_lower_case=True)\n",
    "\n",
    "### punctuation encoder\n",
    "punctuationEnc = {\n",
    "    'SPACE': 0,\n",
    "    'PERIOD': 1,\n",
    "}\n",
    "outputDimension = len(punctuationEnc)\n",
    "\n",
    "### Set Vocabulary Size, Also Specify The BERT Hidden Dimension\n",
    "vocabSize = 32005\n",
    "hiddenDimension = 768\n",
    "\n",
    "### hyper-parameters\n",
    "batchSize = 32\n",
    "sequenceSize = 32\n",
    "learningRate = 1e-5\n",
    "trainLayerIndex = 0\n",
    "numEpo = 3\n",
    "\n",
    "listHyperNames = ['sequenceSize', 'batchSize', 'learningRate', 'trainLayerIndex', 'numEpo']\n",
    "listHyperValues = [str(sequenceSize), str(batchSize), str(learningRate), str(trainLayerIndex), str(numEpo)]\n",
    "time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_path = 'ModelsExpTimeStamps/{}/'.format(time)\n",
    "# os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Training Data ... \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-067abbe4a5fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m### Encode Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mXTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXTrainBeg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXTrainEnd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXTrainGap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencodeDataTimeStamps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpunctuationEnc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m### Insert Target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/PunctuatorBERTTensorFlow2/utils.py\u001b[0m in \u001b[0;36mencodeDataTimeStamps\u001b[0;34m(data, tokenizer, punctuationEnc)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mwordBeg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mwordEnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mgap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtokensIds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "### Get Training Dataset\n",
    "print('\\nProcessing Training Data ... ')\n",
    "\n",
    "# # THIS IN CASE STARTING FROM FILE WITH SENTENCES\n",
    "# # this is the file structured in sentences\n",
    "# trainDataName = './outFile_030.txt'  # file path + name\n",
    "# # from sentences to columns words+punctuation\n",
    "# dataTrain = loadFile(processingScriber(trainDataName))\n",
    "\n",
    "# THIS IN CASE STARTING FROM FILE WITH COLUMNS\n",
    "trainDataName = './AudioFeatures/synTrainSet_3200.txt'\n",
    "# obtain a list of strings, each string is a line of the input file. \n",
    "dataTrain = loadFile(trainDataName)\n",
    "\n",
    "### Encode Data\n",
    "XTrain, XTrainBeg, XTrainEnd, XTrainGap, yTrain = encodeDataTimeStamps(dataTrain, tokenizer, punctuationEnc)\n",
    "\n",
    "### Insert Target\n",
    "XTrainMod, XTrainBegMod, XTrainEndMod, XTrainGapMod = insertTargetTimeStamps(XTrain, XTrainBeg, XTrainEnd, XTrainGap, sequenceSize)\n",
    "\n",
    "### Compute The Cumulative Gaps\n",
    "XTrainGapMod = XTrainGapMod.astype(np.float)\n",
    "XTrainCumGapMod = np.cumsum(XTrainGapMod, axis=1)\n",
    "\n",
    "XTrainAll = np.stack((XTrainMod, XTrainBegMod, XTrainEndMod, XTrainCumGapMod), axis = 2)\n",
    "\n",
    "### Build The Dataset\n",
    "trainDataset = tf.data.Dataset.from_tensor_slices((XTrainAll, yTrain)).batch(batchSize)\n",
    "# trainDataset = tf.data.Dataset.from_tensor_slices((XTrainAll, yTrain)).shuffle(buffer_size=1000000).batch(batchSize)\n",
    "\n",
    "print(\"\\nTraining Dataset Tensor Shape = \", XTrainAll.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build The TS [TimeStamps] Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build The Experimental Model\n",
    "print('\\nBulding the Model ... ')\n",
    "\n",
    "inpA = tf.keras.Input(shape=(sequenceSize), dtype='int32')\n",
    "inpB = tf.keras.Input(shape=(sequenceSize, hiddenDimension), batch_size=batchSize, dtype='float32')\n",
    "x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(inpA, custom_embeds=inpB)[0]\n",
    "x = tf.keras.layers.Reshape((sequenceSize*vocabSize,))(x)\n",
    "out = tf.keras.layers.Dense(len(punctuationEnc))(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[inpA, inpB], outputs=[out])\n",
    "\n",
    "# define the loss\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "def loss(model, x, custom_embeds, y):\n",
    "    y_ = model([x, custom_embeds])\n",
    "    return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "# func to calculate the gradients\n",
    "def grad(model, inputs, custom_embeds, targets, trainLayerIndex):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, custom_embeds, targets)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables[trainLayerIndex:])\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExperiment Folder: \", time)\n",
    "print(\"\\nHyperparameters:\")\n",
    "print('vocabSize = ', vocabSize)\n",
    "print('sequenceSize = ', sequenceSize)\n",
    "print('batchSize = ', batchSize)\n",
    "print('leaRat = ', learningRate)\n",
    "print('Train Layer Index = ', trainLayerIndex)\n",
    "print('numEpo = ', numEpo)\n",
    "\n",
    "epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "checkpoint_path = save_path + \"cp-{epoch:03d}.ckpt\"\n",
    "\n",
    "print(\"\\nTraining the Model ... \")\n",
    "for epoch in range(1, numEpo+1):\n",
    "\n",
    "    # training loop\n",
    "    for x, y in trainDataset:\n",
    "        \n",
    "        tokensTensor = tf.cast(x[:, :, 0], dtype=\"int64\")\n",
    "        beginsTensor = x[:, :, 1]\n",
    "        endsTensor = x[:, :, 2]\n",
    "        cumGapTensor = x[:, :, 3]\n",
    "\n",
    "#         ### get positional encoding tensor for time stamps\n",
    "#         # use both start and end timestamps\n",
    "#         depth = hiddenDimension//2\n",
    "#         inputBeginsTensor = positionalEncoding(beginsTensor, depth)\n",
    "#         inputEndsTensor = positionalEncoding(endsTensor, depth)\n",
    "#         inputPosTensor = tf.convert_to_tensor(np.concatenate((inputBeginsTensor, inputEndsTensor), axis=2))\n",
    "\n",
    "#         ### get positional encoding tensor for time stamps\n",
    "#         # use only start timestamps\n",
    "#         depth = hiddenDimension\n",
    "#         inputBeginsTensor = positionalEncoding(beginsTensor, depth)\n",
    "#         inputPosTensor = tf.convert_to_tensor(inputBeginsTensor)\n",
    "\n",
    "        ### get positional encoding tensor for time stamps\n",
    "        # use only cumulative gaps\n",
    "        depth = hiddenDimension\n",
    "        inputCumGapTensor = positionalEncoding(cumGapTensor, depth)\n",
    "        inputPosTensor = tf.convert_to_tensor(inputCumGapTensor)\n",
    "\n",
    "        # optimize the model\n",
    "        loss_value, grads = grad(model, tokensTensor, inputPosTensor, y, trainLayerIndex)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables[trainLayerIndex:]))\n",
    "\n",
    "        # track progress\n",
    "        epoch_loss_avg.update_state(loss_value)\n",
    "        epoch_accuracy.update_state(y, model([tokensTensor, inputPosTensor]))\n",
    "\n",
    "    # end epoch\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "    print(\"\\nEpoch {:03d}: (Training)   Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch, epoch_loss_avg.result(), epoch_accuracy.result()))\n",
    "\n",
    "    # # save model if new min for train loss is found\n",
    "    tmpTrain = epoch_loss_avg.result().numpy()\n",
    "    model.save_weights(checkpoint_path.format(epoch=epoch))\n",
    "\n",
    "    epoch_loss_avg.reset_states()\n",
    "    epoch_accuracy.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvTrans",
   "language": "python",
   "name": "venvtrans"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
