{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train The Time Stamps Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input file is obtained with the script processAlignedTranscripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()  # silence TF warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import loadFile, encodeDataTimeStamps, correctTimeStamps, insertTargetTimeStamps, positionalEncoding\n",
    "from transformers import AutoTokenizer, TFCamembertForMaskedLM\n",
    "from datetime import datetime\n",
    "import copy\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jplu/tf-camembert-base\", do_lower_case=True)\n",
    "\n",
    "### punctuation encoder\n",
    "punctuationEnc = {\n",
    "    'SPACE': 0,\n",
    "    'PERIOD': 1,\n",
    "}\n",
    "outputDimension = len(punctuationEnc)\n",
    "\n",
    "### Set Vocabulary Size, Also Specify The BERT Hidden Dimension\n",
    "vocabSize = 32005\n",
    "hiddenDimension = 768\n",
    "\n",
    "### hyper-parameters\n",
    "batchSize = 12\n",
    "sequenceSize = 32\n",
    "learningRate = 1e-5\n",
    "trainLayerIndex = 0\n",
    "numEpo = 3\n",
    "\n",
    "listHyperNames = ['sequenceSize', 'batchSize', 'learningRate', 'trainLayerIndex', 'numEpo']\n",
    "listHyperValues = [str(sequenceSize), str(batchSize), str(learningRate), str(trainLayerIndex), str(numEpo)]\n",
    "time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_path = 'ModelsExpTimeStamps/{}/'.format(time)\n",
    "os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Training Data ... \n",
      "\n",
      "Training Dataset Tensor Shape =  (320, 32, 4)\n"
     ]
    }
   ],
   "source": [
    "### Get Training Dataset\n",
    "print('\\nProcessing Training Data ... ')\n",
    "\n",
    "# # THIS IN CASE STARTING FROM FILE WITH SENTENCES\n",
    "# # this is the file structured in sentences\n",
    "# trainDataName = './outFile_030.txt'  # file path + name\n",
    "# # from sentences to columns words+punctuation\n",
    "# dataTrain = loadFile(processingScriber(trainDataName))\n",
    "\n",
    "# THIS IN CASE STARTING FROM FILE WITH COLUMNS\n",
    "trainDataName = './AudioFeatures/syntheticTrainSet_320.txt'\n",
    "# obtain a list of strings, each string is a line of the input file. \n",
    "dataTrain = loadFile(trainDataName)\n",
    "\n",
    "### Encode Data\n",
    "XTrain, XTrainBeg, XTrainEnd, XTrainGap, yTrain = encodeDataTimeStamps(dataTrain, tokenizer, punctuationEnc)\n",
    "\n",
    "### Insert Target\n",
    "XTrainMod, XTrainBegMod, XTrainEndMod, XTrainGapMod = insertTargetTimeStamps(XTrain, XTrainBeg, XTrainEnd, XTrainGap, sequenceSize)\n",
    "\n",
    "### Compute The Cumulative Gaps\n",
    "XTrainGapMod = XTrainGapMod.astype(np.float)\n",
    "XTrainCumGapMod = np.cumsum(XTrainGapMod, axis=1)\n",
    "\n",
    "XTrainAll = np.stack((XTrainMod, XTrainBegMod, XTrainEndMod, XTrainCumGapMod), axis = 2)\n",
    "\n",
    "### Build The Dataset\n",
    "trainDataset = tf.data.Dataset.from_tensor_slices((XTrainAll, yTrain)).batch(batchSize)\n",
    "# trainDataset = tf.data.Dataset.from_tensor_slices((XTrainAll, yTrain)).shuffle(buffer_size=1000000).batch(batchSize)\n",
    "\n",
    "print(\"\\nTraining Dataset Tensor Shape = \", XTrainAll.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 32, 4)\n"
     ]
    }
   ],
   "source": [
    "print(XTrainAll.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 32)\n",
      "XTrainMod       =  [5061 5061 5061 5061 5061 5061 5061 5061 5061 5061 5061 5061 5061 5061\n",
      " 5061    0 5061 5061 5061 5061 5061 5061 5061 5061 5061 5061 5061 5061\n",
      " 5061 5061 5061 5061]\n",
      "XTRainBegMod =  [32.4179 34.2816 34.5806 35.692  36.7654 37.5451 39.3642 40.1335 41.0355\n",
      " 42.8361 44.6289 45.6154 46.6864 47.3879 48.7507 48.7507 49.8575 51.0997\n",
      " 52.0449 53.3432 53.7914 55.3459 55.8615 57.3213 58.6349 60.1355 61.5558\n",
      " 62.5683 63.2566 63.827  65.3094 66.4506]\n",
      "XTRainCumGapMod =  [ 0.211   0.9603  0.9603  1.3626  2.1542  2.4895  3.1934  3.3486  3.5809\n",
      "  4.3492  4.9995  5.1651  5.357   5.8012  6.1253  6.1253  6.3191  7.1897\n",
      "  7.391   7.6635  7.8253  8.2752  8.4356  9.3737  9.56   10.0021 10.3851\n",
      " 10.8213 11.2536 11.5193 11.909  12.4665]\n"
     ]
    }
   ],
   "source": [
    "print(XTrainCumGapMod.shape)\n",
    "print(\"XTrainMod       = \",  XTrainMod[300, :])\n",
    "print(\"XTRainBegMod = \",  XTrainBegMod[300, :])\n",
    "print(\"XTRainCumGapMod = \",  XTrainCumGapMod[300, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2576, 26, 603, 276, 7125]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"saint-jaurès\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2576, 26, 603, 276, 7125, 145, 15, 223, 42, 13]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"saint-jaurès donc à voir avec la\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starts    11.0348 12.1058 12.8073 14.1701 14.1701 15.2769 16.5191 17.4643 18.7626 19.2108\n",
    "# Gaps      0.1656  0.1919  0.4442  0.3241  0.00000 0.1938  0.8706  0.2013  0.2725  0.1618 \n",
    "# CumGaps   0.1656  0.3575  0.8017  1.1258  1.1258  1.3196  2.1902  2.3915  2.664    2.8257999999999996 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starts   4.9924  5.8262  6.6361  8.3279  8.3279  9.6607  11.4461  12.4521  13.1867  14.4111\n",
    "# Gaps     0.1829  0.2304  0.3291  0.6842  0.0000  0.3190  0.8313   0.2588   0.4407   0.4400\n",
    "# CumGaps  0.1829  0.4133  0.7424  1.4266  1.4266  1.7456  2.5769   2.8357   3.2764   3.7164       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  saint-jaurès     donc  à   voir  avec  la\n",
    "#  26 603 276 7125  145   15  223   42    13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  tokens_ids  26      603     276     7125    0       145     15      223     42      13\n",
    "#  Gap         0.0000  0.0000  0.0000  0.1000  0.0000  0.9700  0.7000  0.0000  0.0800  0.0400\n",
    "#  cumGap      0.0000  0.0000  0.0000  0.1000  0.1000  1.0700  1.7700  1.7700  1.8500  1.8900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build The TS [TimeStamps] Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bulding the Model ... \n"
     ]
    }
   ],
   "source": [
    "### Build The Experimental Model\n",
    "print('\\nBulding the Model ... ')\n",
    "\n",
    "inpA = tf.keras.Input(shape=(sequenceSize), dtype='int32')\n",
    "inpB = tf.keras.Input(shape=(sequenceSize, hiddenDimension), batch_size=batchSize, dtype='float32')\n",
    "x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(inpA, custom_embeds=inpB)[0]\n",
    "x = tf.keras.layers.Reshape((sequenceSize*vocabSize,))(x)\n",
    "out = tf.keras.layers.Dense(len(punctuationEnc))(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[inpA, inpB], outputs=[out])\n",
    "\n",
    "# define the loss\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "def loss(model, x, custom_embeds, y):\n",
    "    y_ = model([x, custom_embeds])\n",
    "    return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "# func to calculate the gradients\n",
    "def grad(model, inputs, custom_embeds, targets, trainLayerIndex):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, custom_embeds, targets)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables[trainLayerIndex:])\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment Folder:  20200722_114849\n",
      "\n",
      "Hyperparameters:\n",
      "vocabSize =  32005\n",
      "sequenceSize =  32\n",
      "batchSize =  12\n",
      "leaRat =  1e-05\n",
      "Train Layer Index =  0\n",
      "numEpo =  3\n",
      "\n",
      "Training the Model ... \n",
      "\n",
      "Epoch 001: (Training)   Loss: 6.922, Accuracy: 67.813%\n",
      "\n",
      "Epoch 002: (Training)   Loss: 1.570, Accuracy: 68.750%\n",
      "\n",
      "Epoch 003: (Training)   Loss: 0.678, Accuracy: 74.063%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExperiment Folder: \", time)\n",
    "print(\"\\nHyperparameters:\")\n",
    "print('vocabSize = ', vocabSize)\n",
    "print('sequenceSize = ', sequenceSize)\n",
    "print('batchSize = ', batchSize)\n",
    "print('leaRat = ', learningRate)\n",
    "print('Train Layer Index = ', trainLayerIndex)\n",
    "print('numEpo = ', numEpo)\n",
    "\n",
    "epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "checkpoint_path = save_path + \"cp-{epoch:03d}.ckpt\"\n",
    "\n",
    "print(\"\\nTraining the Model ... \")\n",
    "for epoch in range(1, numEpo+1):\n",
    "\n",
    "    # training loop\n",
    "    for x, y in trainDataset:\n",
    "        \n",
    "        tokensTensor = tf.cast(x[:, :, 0], dtype=\"int64\")\n",
    "        beginsTensor = x[:, :, 1]\n",
    "        endsTensor = x[:, :, 2]\n",
    "        cumGapTensor = x[:, :, 3]\n",
    "\n",
    "#         ### get positional encoding tensor for time stamps\n",
    "#         # use both start and end timestamps\n",
    "#         depth = hiddenDimension//2\n",
    "#         inputBeginsTensor = positionalEncoding(beginsTensor, depth)\n",
    "#         inputEndsTensor = positionalEncoding(endsTensor, depth)\n",
    "#         inputPosTensor = tf.convert_to_tensor(np.concatenate((inputBeginsTensor, inputEndsTensor), axis=2))\n",
    "\n",
    "#         ### get positional encoding tensor for time stamps\n",
    "#         # use only start timestamps\n",
    "#         depth = hiddenDimension\n",
    "#         inputBeginsTensor = positionalEncoding(beginsTensor, depth)\n",
    "#         inputPosTensor = tf.convert_to_tensor(inputBeginsTensor)\n",
    "\n",
    "        ### get positional encoding tensor for time stamps\n",
    "        # use only cumulative gaps\n",
    "        depth = hiddenDimension\n",
    "        inputCumGapTensor = positionalEncoding(cumGapTensor, depth)\n",
    "        inputPosTensor = tf.convert_to_tensor(inputCumGapTensor)\n",
    "\n",
    "        # optimize the model\n",
    "        loss_value, grads = grad(model, tokensTensor, inputPosTensor, y, trainLayerIndex)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables[trainLayerIndex:]))\n",
    "\n",
    "        # track progress\n",
    "        epoch_loss_avg.update_state(loss_value)\n",
    "        epoch_accuracy.update_state(y, model([tokensTensor, inputPosTensor]))\n",
    "\n",
    "    # end epoch\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "    print(\"\\nEpoch {:03d}: (Training)   Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch, epoch_loss_avg.result(), epoch_accuracy.result()))\n",
    "\n",
    "    # # save model if new min for train loss is found\n",
    "    tmpTrain = epoch_loss_avg.result().numpy()\n",
    "    model.save_weights(checkpoint_path.format(epoch=epoch))\n",
    "\n",
    "    epoch_loss_avg.reset_states()\n",
    "    epoch_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# march = 2584.49\n",
    "# june = 3100.29\n",
    "# dif = abs(march-june)\n",
    "\n",
    "# perInc = dif / march * 100\n",
    "# print(perInc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvTrans",
   "language": "python",
   "name": "venvtrans"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
